# Building a Neural Machine Translation System: Urdu Poetry to Roman Urdu

*A comprehensive journey through implementing a BiLSTM-based NMT system from scratch*

---

## Introduction

In the realm of Natural Language Processing, Neural Machine Translation (NMT) represents one of the most fascinating applications of deep learning. This blog post chronicles my journey of building a complete NMT system to translate Urdu poetry into Roman Urdu, implementing everything from scratch including tokenization, model architecture, and evaluation metrics.

## The Challenge

The task was to create a sequence-to-sequence model using a bidirectional LSTM (BiLSTM) encoder and LSTM decoder to translate Urdu text into its Roman Urdu transliteration. The challenge involved:

- Working with poetic text from the `urdu_ghazals_rekhta` dataset
- Implementing Byte-Pair Encoding (BPE) tokenization from scratch
- Building a robust BiLSTM encoder with attention mechanism
- Creating a comprehensive evaluation framework
- Conducting systematic hyperparameter experiments

## Dataset and Preprocessing

### Dataset Overview
The project utilized the `urdu_ghazals_rekhta` dataset, which contains poetic works (Ghazals) in multiple scripts:
- **Urdu script** (source language)
- **English transliteration** (target language - Roman Urdu)
- **Hindi script** (not used in this project)

The dataset includes works from renowned poets like Mirza Ghalib, Allama Iqbal, Ahmad Faraz, and many others, providing a rich collection of classical and modern Urdu poetry.

### Data Preprocessing Pipeline

```python
class TextCleaner:
    @staticmethod
    def clean_urdu(text: str) -> str:
        text = unicodedata.normalize('NFKC', text)
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'[^\u0600-\u06FF\u0750-\u077F\u08A0-\u08FF\s€îÿåÿçÿéÿèÿü!]', '', text)
        return text.strip()
    
    @staticmethod
    def clean_roman(text: str) -> str:
        text = text.lower()
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'[^a-zA-ZƒÅƒ´≈´ƒÄƒ™≈™√±·πá·πõ·π≠·∏ç·π£ƒ°·∏•·∏≥·∫ì·∫ï\s\'\-\.]', '', text)
        return text.strip()
```

**Key preprocessing steps:**
1. **Unicode normalization** using NFKC for consistent character representation
2. **Whitespace normalization** to handle inconsistent spacing
3. **Character filtering** to retain only relevant script characters
4. **Length filtering** to remove extremely short or long sequences (2-50 words)
5. **Data splitting** into 50% training, 25% validation, and 25% test sets

## Tokenization: Byte-Pair Encoding from Scratch

One of the most challenging aspects was implementing BPE tokenization from scratch, as per the assignment requirements. This was crucial for handling the morphological complexity of Urdu text.

### BPE Implementation

```python
class BPETokenizer:
    def __init__(self, vocab_size: int = 10000):
        self.vocab_size = vocab_size
        self.word_freqs = Counter()
        self.vocab = {}
        self.merges = []
        self.special_tokens = ['<pad>', '<unk>', '<sos>', '<eos>']
    
    def train(self, texts):
        # Initialize with character-level vocabulary
        for text in texts:
            words = text.split()
            for word in words:
                self.word_freqs[word] += 1
        
        vocab = {}
        for word, freq in self.word_freqs.items():
            vocab[' '.join(list(word)) + ' </w>'] = freq
        
        # Perform BPE merges
        num_merges = self.vocab_size - len(self.special_tokens)
        for i in range(num_merges):
            pairs = self._get_stats(vocab)
            if not pairs:
                break
            best_pair = max(pairs, key=pairs.get)
            vocab = self._merge_vocab(best_pair, vocab)
            self.merges.append(best_pair)
```

**BPE Benefits:**
- **Subword representation** handles out-of-vocabulary words effectively
- **Morphological awareness** captures Urdu's rich morphology
- **Vocabulary efficiency** reduces vocabulary size while maintaining coverage
- **Consistent tokenization** across different text lengths

## Model Architecture

### BiLSTM Encoder

The encoder uses a bidirectional LSTM to capture both forward and backward context in the source sequence:

```python
class BiLSTMEncoder(nn.Module):
    def __init__(self, vocab_size: int, embed_dim: int, hidden_dim: int, 
                 num_layers: int = 2, dropout: float = 0.3):
        super(BiLSTMEncoder, self).__init__()
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, 
                           batch_first=True, bidirectional=True, dropout=dropout)
        self.dropout = nn.Dropout(dropout)
```

**Key features:**
- **Bidirectional processing** captures both past and future context
- **Multiple layers** (1-4 layers tested) for hierarchical feature learning
- **Dropout regularization** to prevent overfitting
- **Packed sequences** for efficient processing of variable-length inputs

### Attention Mechanism

The attention mechanism allows the decoder to focus on relevant parts of the source sequence:

```python
class AttentionMechanism(nn.Module):
    def __init__(self, decoder_hidden_dim: int, encoder_hidden_dim: int):
        super(AttentionMechanism, self).__init__()
        self.decoder_projection = nn.Linear(decoder_hidden_dim, encoder_hidden_dim)
        self.encoder_projection = nn.Linear(encoder_hidden_dim, encoder_hidden_dim)
        self.attention_projection = nn.Linear(encoder_hidden_dim, 1)
    
    def forward(self, decoder_hidden, encoder_outputs, mask=None):
        # Calculate attention scores
        energy = torch.tanh(decoder_proj + encoder_proj)
        attention_scores = self.attention_projection(energy).squeeze(2)
        
        if mask is not None:
            attention_scores = attention_scores.masked_fill(mask == 0, -1e10)
        
        attention_weights = F.softmax(attention_scores, dim=1)
        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)
        
        return context, attention_weights
```

### LSTM Decoder

The decoder generates the target sequence using the encoder's context and attention:

```python
class LSTMDecoder(nn.Module):
    def __init__(self, vocab_size: int, embed_dim: int, hidden_dim: int, 
                 encoder_hidden_dim: int, num_layers: int = 4, dropout: float = 0.3):
        super(LSTMDecoder, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.attention = AttentionMechanism(hidden_dim, encoder_hidden_dim)
        self.lstm = nn.LSTM(embed_dim + encoder_hidden_dim, hidden_dim, 
                           num_layers, batch_first=True, dropout=dropout)
        self.output_projection = nn.Linear(hidden_dim, vocab_size)
```

## Training and Optimization

### Training Strategy

The training process employed several key techniques:

1. **Teacher Forcing**: During training, the decoder receives the ground truth target sequence
2. **Gradient Clipping**: Prevented exploding gradients with max_norm=1.0
3. **Learning Rate Scheduling**: ReduceLROnPlateau for adaptive learning rate adjustment
4. **Early Stopping**: Prevented overfitting with patience-based stopping
5. **Checkpointing**: Saved model states for recovery and best model selection

### Loss Function and Optimization

```python
criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding tokens
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)
```

## Hyperparameter Experiments

A systematic approach was taken to explore the hyperparameter space as required by the assignment:

### Experiment Configurations

| Experiment | Embed Dim | Hidden Dim | Encoder Layers | Decoder Layers | Dropout | Learning Rate | Batch Size |
|------------|-----------|------------|----------------|----------------|---------|---------------|------------|
| Small Model | 128 | 256 | 1 | 2 | 0.1 | 1e-3 | 32 |
| Medium Model | 256 | 512 | 2 | 3 | 0.3 | 5e-4 | 64 |
| Large Model | 512 | 512 | 3 | 4 | 0.5 | 1e-4 | 128 |

### Experimental Results

The experiments revealed interesting insights:

1. **Model Size vs Performance**: Larger models didn't always perform better, indicating the importance of regularization
2. **Dropout Impact**: Higher dropout (0.5) helped prevent overfitting in larger models
3. **Learning Rate Sensitivity**: Lower learning rates (1e-4) were crucial for stable training of larger models
4. **Batch Size Effects**: Larger batch sizes provided more stable gradients but required careful learning rate tuning

## Evaluation Metrics

### Comprehensive Evaluation Framework

The evaluation system included multiple metrics to assess different aspects of translation quality:

1. **Word-Level Accuracy**: Percentage of correctly predicted words
2. **BLEU Score**: Standard machine translation quality metric
3. **Character Error Rate (CER)**: Character-level accuracy using Levenshtein distance
4. **Perplexity**: Model's confidence in predictions
5. **Qualitative Examples**: Human-readable translation samples

### Sample Results

```
üìä EXPERIMENTATION RESULTS SUMMARY
================================================================================

Experiment           Val Acc    Test Acc   Val BLEU   Test BLEU  Val CER    Test CER   
----------------------------------------------------------------------------------------------------
Experiment 1: Small  75.23      74.89      12.45      11.98      0.1234     0.1289    
Experiment 2: Medium 82.45      81.67      18.23      17.45      0.0987     0.1023    
Experiment 3: Large  79.12      78.34      15.67      14.89      0.1123     0.1156    

üèÜ BEST MODEL: Experiment 2: Medium Model
   Validation Accuracy: 82.45%
```

## Technical Challenges and Solutions

### 1. GPU/CPU Tensor Compatibility
**Challenge**: PyTorch's `pack_padded_sequence` requires CPU tensors for lengths
**Solution**: Explicitly moved length tensors to CPU before packing

```python
if lengths is not None:
    lengths_cpu = lengths.cpu() if lengths.is_cuda else lengths
    packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths_cpu, batch_first=True, enforce_sorted=False)
```

### 2. Attention Mechanism Dimension Mismatches
**Challenge**: Complex tensor shape handling in attention computation
**Solution**: Careful dimension management and broadcasting

### 3. Memory Optimization
**Challenge**: Large models consuming excessive GPU memory
**Solution**: Implemented checkpointing and minimal dataset options for development

### 4. Text Decoding Issues
**Challenge**: Converting token IDs back to readable text
**Solution**: Implemented robust decode methods with proper special token handling

## Time-Saving Optimizations

To address the challenge of long training times in Colab, several optimizations were implemented:

### 1. Checkpoint System
```python
class TokenizerCheckpoint:
    def save_tokenizers(self, src_tokenizer, tgt_tokenizer, train_pairs, vocab_sizes, config):
        # Save tokenizers to avoid retraining (saves 15-20 minutes)
        
class ModelCheckpoint:
    def save_checkpoint(self, epoch, model, optimizer, ...):
        # Save model states every 2 epochs for recovery
```

### 2. Minimal Dataset Testing
- Option to train on subset (2000 pairs) for quick validation
- Full dataset training for final results

### 3. Google Drive Integration
- Persistent storage across Colab sessions
- Automatic mounting and file management

## Inference and Deployment

### Simple Translation Function

```python
def translate_urdu_poetry(model, src_tokenizer, tgt_tokenizer, urdu_text, device='cuda', max_length=100):
    """
    Translate Urdu poetry to Roman Urdu using trained model
    """
    model.eval()
    
    # Clean and tokenize input
    cleaned_urdu = cleaner.clean_urdu(urdu_text)
    src_tokens = src_tokenizer.encode(cleaned_urdu)
    
    # Generate translation using greedy decoding
    target_sequence = [sos_token]
    with torch.no_grad():
        for step in range(max_length):
            outputs = model(src_tensor, target_tensor, src_lengths, teacher_forcing_ratio=0.0)
            next_token = torch.argmax(outputs[0, -1, :], dim=-1).item()
            
            if next_token == eos_token:
                break
            target_sequence.append(next_token)
    
    # Decode and return results
    generated_text = tgt_tokenizer.decode(target_sequence)
    return {
        'source_urdu': urdu_text,
        'translation': generated_text,
        'confidence': confidence,
        'tokens_generated': len(target_sequence)
    }
```

### Usage Example

```python
# Load the best trained model
model, config = load_trained_model('best_model_exp_2.pth')

# Translate Urdu poetry
result = translate_urdu_poetry(
    model, src_tokenizer, tgt_tokenizer, 
    "€å€Å ŸÜ€Å ÿ™⁄æ€å €ÅŸÖÿßÿ±€å ŸÇÿ≥ŸÖÿ™ ⁄©€Å ŸàÿµÿßŸÑ €åÿßÿ± €ÅŸàÿ™ÿß"
)

print(f"Translation: {result['translation']}")
print(f"Confidence: {result['confidence']:.3f}")
```

## Key Learnings and Insights

### 1. Data Quality is Paramount
- Proper text cleaning and normalization significantly improved model performance
- Consistent data splits across experiments ensured fair comparison

### 2. Architecture Matters
- BiLSTM encoder effectively captured bidirectional context
- Attention mechanism was crucial for handling long sequences
- Proper layer initialization and dropout were essential for training stability

### 3. Hyperparameter Sensitivity
- Learning rate had the most significant impact on training stability
- Dropout was crucial for preventing overfitting in larger models
- Batch size affected both training speed and model performance

### 4. Evaluation Complexity
- Multiple metrics provided different perspectives on model quality
- Qualitative evaluation was essential for understanding model behavior
- BLEU scores were lower than expected, indicating the challenge of poetic translation

## Future Improvements

### 1. Model Architecture Enhancements
- **Transformer-based models** for better long-range dependencies
- **Multi-head attention** for more sophisticated attention patterns
- **Beam search decoding** for better translation quality

### 2. Data Augmentation
- **Back-translation** to increase training data
- **Noise injection** for robustness
- **Synthetic data generation** for rare patterns

### 3. Advanced Training Techniques
- **Curriculum learning** starting with easier examples
- **Multi-task learning** incorporating additional objectives
- **Adversarial training** for better generalization

### 4. Domain-Specific Optimizations
- **Poetry-specific preprocessing** to preserve meter and rhyme
- **Cultural context awareness** for better translations
- **Multi-script support** for comprehensive Urdu processing

## Conclusion

This project demonstrated the complete pipeline of building a Neural Machine Translation system from scratch. The journey from raw text data to a deployable translation model involved numerous challenges and learning opportunities.

**Key Achievements:**
- ‚úÖ Implemented BPE tokenization from scratch
- ‚úÖ Built a robust BiLSTM-based NMT architecture
- ‚úÖ Conducted systematic hyperparameter experiments
- ‚úÖ Created comprehensive evaluation framework
- ‚úÖ Developed time-saving optimization techniques
- ‚úÖ Built inference system for real-world usage

**Final Results:**
- Best model achieved **82.45% validation accuracy**
- Successfully translated Urdu poetry to Roman Urdu
- Demonstrated the effectiveness of attention-based NMT for low-resource languages

The project not only met the assignment requirements but also provided valuable insights into the complexities of neural machine translation, particularly for poetic and morphologically rich languages like Urdu.

**Code Repository**: All code is available in the project repository with detailed documentation and usage examples.

**Deployment**: The trained model can be easily deployed using the provided inference functions for real-world Urdu poetry translation applications.

---

*This blog post represents a comprehensive overview of the Neural Machine Translation project completed as part of the NLP course assignment. The project demonstrates practical implementation of deep learning techniques for natural language processing tasks.*
