{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380c70e2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## this is my final code, in which all model traiinng is done and its error free and running accurately "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a62e7f2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COMPLETE ERROR-FREE GOOGLE COLAB SOLUTION\n",
    "# Neural Machine Translation: Urdu to Roman Urdu\n",
    "# =============================================================================\n",
    "\n",
    "# CELL 1: Install packages\n",
    "\n",
    "%pip install torch torchtext nltk sacrebleu editdistance streamlit\n",
    "%pip install matplotlib seaborn tqdm pandas numpy\n",
    "\n",
    "\n",
    "\n",
    "# CELL 2: Import libraries\n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pickle\n",
    "import hashlib\n",
    "from google.colab import drive\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "from typing import List, Tuple, Dict, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "\n",
    "# CELL 2.5: Time-Saving Checkpoint System\n",
    "\n",
    "# =============================================================================\n",
    "# TIME-SAVING CHECKPOINT SYSTEM\n",
    "# Saves 15-20 minutes of tokenization time and training progress\n",
    "# =============================================================================\n",
    "\n",
    "class TokenizerCheckpoint:\n",
    "    \n",
    "    def __init__(self, checkpoint_dir: str = \"/content/tokenizer_checkpoints\"):\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    def _get_data_hash(self, train_pairs, vocab_sizes):\n",
    "        data_str = f\"{len(train_pairs)}_{vocab_sizes['src']}_{vocab_sizes['tgt']}\"\n",
    "        # Add hash of first few pairs to detect content changes\n",
    "        sample_pairs = str(train_pairs[:10]) if len(train_pairs) > 10 else str(train_pairs)\n",
    "        data_str += hashlib.md5(sample_pairs.encode()).hexdigest()[:8]\n",
    "        return data_str\n",
    "    \n",
    "    def save_tokenizers(self, src_tokenizer, tgt_tokenizer, train_pairs, vocab_sizes, config):\n",
    "        data_hash = self._get_data_hash(train_pairs, vocab_sizes)\n",
    "        checkpoint_path = os.path.join(self.checkpoint_dir, f\"tokenizers_{data_hash}.pkl\")\n",
    "        \n",
    "        checkpoint_data = {\n",
    "            'src_tokenizer': src_tokenizer,\n",
    "            'tgt_tokenizer': tgt_tokenizer,\n",
    "            'vocab_sizes': vocab_sizes,\n",
    "            'config': config,\n",
    "            'data_hash': data_hash,\n",
    "            'num_pairs': len(train_pairs)\n",
    "        }\n",
    "        \n",
    "        with open(checkpoint_path, 'wb') as f:\n",
    "            pickle.dump(checkpoint_data, f)\n",
    "        \n",
    "        print(f\"üíæ Tokenizers saved to: {checkpoint_path}\")\n",
    "        return checkpoint_path\n",
    "    \n",
    "    def load_tokenizers(self, train_pairs, vocab_sizes):\n",
    "        data_hash = self._get_data_hash(train_pairs, vocab_sizes)\n",
    "        checkpoint_path = os.path.join(self.checkpoint_dir, f\"tokenizers_{data_hash}.pkl\")\n",
    "        \n",
    "        if os.path.exists(checkpoint_path):\n",
    "            try:\n",
    "                with open(checkpoint_path, 'rb') as f:\n",
    "                    checkpoint_data = pickle.load(f)\n",
    "                \n",
    "                print(f\"üìÇ Loading tokenizers from: {checkpoint_path}\")\n",
    "                print(f\"üìä Checkpoint info: {checkpoint_data['num_pairs']} pairs, \"\n",
    "                      f\"src_vocab={checkpoint_data['vocab_sizes']['src']}, \"\n",
    "                      f\"tgt_vocab={checkpoint_data['vocab_sizes']['tgt']}\")\n",
    "                \n",
    "                return (checkpoint_data['src_tokenizer'], \n",
    "                       checkpoint_data['tgt_tokenizer'], \n",
    "                       True)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error loading checkpoint: {e}\")\n",
    "                return None, None, False\n",
    "        else:\n",
    "            print(f\"üîç No checkpoint found for data hash: {data_hash}\")\n",
    "            return None, None, False\n",
    "\n",
    "class ModelCheckpoint:\n",
    "    \n",
    "    def __init__(self, checkpoint_dir: str = \"/content/model_checkpoints\"):\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    def save_checkpoint(self, epoch, model, optimizer, train_losses, val_losses, \n",
    "                       train_perplexities, val_perplexities, best_val_loss, config):\n",
    "        checkpoint_path = os.path.join(self.checkpoint_dir, f\"checkpoint_epoch_{epoch}.pth\")\n",
    "        \n",
    "        checkpoint_data = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'train_perplexities': train_perplexities,\n",
    "            'val_perplexities': val_perplexities,\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'config': config\n",
    "        }\n",
    "        \n",
    "        torch.save(checkpoint_data, checkpoint_path)\n",
    "        print(f\"üíæ Model checkpoint saved: {checkpoint_path}\")\n",
    "        return checkpoint_path\n",
    "    \n",
    "    def load_checkpoint(self, model, optimizer, checkpoint_path):\n",
    "        if os.path.exists(checkpoint_path):\n",
    "            checkpoint = torch.load(checkpoint_path, map_location='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            \n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            \n",
    "            print(f\"üìÇ Model checkpoint loaded: {checkpoint_path}\")\n",
    "            print(f\"üìä Resuming from epoch: {checkpoint['epoch']}\")\n",
    "            \n",
    "            return checkpoint\n",
    "        else:\n",
    "            print(f\"‚ùå Checkpoint not found: {checkpoint_path}\")\n",
    "            return None\n",
    "\n",
    "def create_minimal_dataset(dataset_path, max_pairs=1000):\n",
    "    print(f\"üöÄ Creating minimal dataset (max {max_pairs} pairs)...\")\n",
    "    \n",
    "    urdu_texts, roman_texts = load_dataset(dataset_path)\n",
    "    \n",
    "    # Take only a subset for quick testing\n",
    "    subset_size = min(max_pairs, len(urdu_texts))\n",
    "    urdu_subset = urdu_texts[:subset_size]\n",
    "    roman_subset = roman_texts[:subset_size]\n",
    "    \n",
    "    print(f\"üìä Minimal dataset: {len(urdu_subset)} pairs (from {len(urdu_texts)} total)\")\n",
    "    \n",
    "    return urdu_subset, roman_subset\n",
    "\n",
    "def mount_google_drive():\n",
    "    try:\n",
    "        drive.mount('/content/drive')\n",
    "        print(\"‚úÖ Google Drive mounted successfully!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not mount Google Drive: {e}\")\n",
    "        return False\n",
    "\n",
    "def create_tokenizers_with_checkpoint(train_pairs, config, checkpoint_system):\n",
    "    print(\"\\\\nüîß Creating tokenizers with checkpoint system...\")\n",
    "    \n",
    "    vocab_sizes = {\n",
    "        'src': config['src_vocab_size'],\n",
    "        'tgt': config['tgt_vocab_size']\n",
    "    }\n",
    "    \n",
    "    # Try to load from checkpoint first\n",
    "    src_tokenizer, tgt_tokenizer, loaded = checkpoint_system.load_tokenizers(train_pairs, vocab_sizes)\n",
    "    \n",
    "    if loaded:\n",
    "        print(\"‚úÖ Tokenizers loaded from checkpoint - saving 15+ minutes!\")\n",
    "        return src_tokenizer, tgt_tokenizer\n",
    "    else:\n",
    "        print(\"üîÑ No checkpoint found, training new tokenizers...\")\n",
    "        \n",
    "        # Train new tokenizers\n",
    "        src_tokenizer, tgt_tokenizer = create_tokenizers(\n",
    "            train_pairs,\n",
    "            src_vocab_size=config['src_vocab_size'],\n",
    "            tgt_vocab_size=config['tgt_vocab_size']\n",
    "        )\n",
    "        \n",
    "        # Save checkpoint for next time\n",
    "        checkpoint_system.save_tokenizers(src_tokenizer, tgt_tokenizer, train_pairs, vocab_sizes, config)\n",
    "        \n",
    "        return src_tokenizer, tgt_tokenizer\n",
    "\n",
    "\n",
    "# CELL 3: Dataset Setup with Extraction\n",
    "\n",
    "import zipfile\n",
    "\n",
    "# Clone repository\n",
    "!git clone https://github.com/amir9ume/urdu_ghazals_rekhta.git\n",
    "\n",
    "print(\"üîß EXTRACTING DATASET FROM ZIP FILE...\")\n",
    "\n",
    "# Extract dataset\n",
    "zip_path = '/content/urdu_ghazals_rekhta/dataset/dataset.zip'\n",
    "extract_to = '/content/dataset_extracted'\n",
    "\n",
    "if not os.path.exists(extract_to):\n",
    "    print(f\"üì¶ Extracting {zip_path}\")\n",
    "    os.makedirs(extract_to, exist_ok=True)\n",
    "    \n",
    "    if os.path.exists(zip_path):\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_to)\n",
    "        print(\"‚úÖ Extraction completed!\")\n",
    "    else:\n",
    "        print(f\"‚ùå Zip file not found at {zip_path}\")\n",
    "else:\n",
    "    print(\"‚úÖ Dataset already extracted!\")\n",
    "\n",
    "# Find dataset path (avoid __MACOSX)\n",
    "dataset_path = None\n",
    "for root, dirs, files in os.walk(extract_to):\n",
    "    if '__MACOSX' in root:\n",
    "        continue\n",
    "    if any(poet in dirs for poet in ['mirza-ghalib', 'ahmad-faraz', 'allama-iqbal']):\n",
    "        dataset_path = root\n",
    "        break\n",
    "\n",
    "if dataset_path:\n",
    "    print(f\"üéØ Dataset found at: {dataset_path}\")\n",
    "    poets = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
    "    print(f\"üìö Found {len(poets)} poets\")\n",
    "else:\n",
    "    print(\"‚ùå Could not find dataset!\")\n",
    "\n",
    "print(f\"‚úÖ Final dataset path: {dataset_path}\")\n",
    "\n",
    "\n",
    "# CELL 4: Data Loader with Text Cleaning\n",
    "\n",
    "import unicodedata\n",
    "\n",
    "class TextCleaner:\n",
    "    @staticmethod\n",
    "    def clean_urdu(text: str) -> str:\n",
    "        text = unicodedata.normalize('NFKC', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'[^\\u0600-\\u06FF\\u0750-\\u077F\\u08A0-\\u08FF\\s€îÿåÿçÿéÿèÿü!]', '', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    @staticmethod\n",
    "    def clean_roman(text: str) -> str:\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'[^a-zA-ZƒÅƒ´≈´ƒÄƒ™≈™√±·πá·πõ·π≠·∏ç·π£ƒ°·∏•·∏≥·∫ì·∫ï\\s\\'\\-\\.]', '', text)\n",
    "        return text.strip()\n",
    "\n",
    "def load_dataset(data_path):\n",
    "    print(f\"üìñ Loading dataset from: {data_path}\")\n",
    "    \n",
    "    urdu_texts = []\n",
    "    roman_texts = []\n",
    "    \n",
    "    poets = [d for d in os.listdir(data_path) \n",
    "             if os.path.isdir(os.path.join(data_path, d)) and not d.startswith('.')]\n",
    "    \n",
    "    print(f\"üìö Found {len(poets)} poets in dataset\")\n",
    "    \n",
    "    for poet in tqdm(poets, desc=\"Loading poets\"):\n",
    "        poet_path = os.path.join(data_path, poet)\n",
    "        urdu_path = os.path.join(poet_path, 'ur')\n",
    "        english_path = os.path.join(poet_path, 'en')\n",
    "        \n",
    "        if os.path.exists(urdu_path) and os.path.exists(english_path):\n",
    "            urdu_files = set(os.listdir(urdu_path))\n",
    "            english_files = set(os.listdir(english_path))\n",
    "            common_files = urdu_files.intersection(english_files)\n",
    "            \n",
    "            for file_name in common_files:\n",
    "                try:\n",
    "                    with open(os.path.join(urdu_path, file_name), 'r', encoding='utf-8') as f:\n",
    "                        urdu_content = f.read().strip()\n",
    "                    \n",
    "                    with open(os.path.join(english_path, file_name), 'r', encoding='utf-8') as f:\n",
    "                        roman_content = f.read().strip()\n",
    "                    \n",
    "                    urdu_lines = [line.strip() for line in urdu_content.split('\\n') if line.strip()]\n",
    "                    roman_lines = [line.strip() for line in roman_content.split('\\n') if line.strip()]\n",
    "                    \n",
    "                    min_lines = min(len(urdu_lines), len(roman_lines))\n",
    "                    for i in range(min_lines):\n",
    "                        if urdu_lines[i] and roman_lines[i]:\n",
    "                            urdu_texts.append(urdu_lines[i])\n",
    "                            roman_texts.append(roman_lines[i])\n",
    "                except:\n",
    "                    continue\n",
    "    \n",
    "    print(f\"\\nDataset loaded:\")\n",
    "    print(f\"Total pairs: {len(urdu_texts)}\")\n",
    "    return urdu_texts, roman_texts\n",
    "\n",
    "def clean_and_split_data(urdu_texts, roman_texts, train_ratio=0.5, val_ratio=0.25, test_ratio=0.25):\n",
    "    if len(urdu_texts) == 0:\n",
    "        return [], [], []\n",
    "    \n",
    "    cleaner = TextCleaner()\n",
    "    \n",
    "    print(\"üßπ Cleaning texts...\")\n",
    "    cleaned_urdu = [cleaner.clean_urdu(text) for text in tqdm(urdu_texts, desc=\"Cleaning Urdu\")]\n",
    "    cleaned_roman = [cleaner.clean_roman(text) for text in tqdm(roman_texts, desc=\"Cleaning Roman\")]\n",
    "    \n",
    "    valid_pairs = []\n",
    "    for u, r in zip(cleaned_urdu, cleaned_roman):\n",
    "        if u and r and 2 <= len(u.split()) <= 50 and 2 <= len(r.split()) <= 50:\n",
    "            valid_pairs.append((u, r))\n",
    "    \n",
    "    print(f\"After cleaning and filtering: {len(valid_pairs)} valid pairs\")\n",
    "    \n",
    "    if len(valid_pairs) == 0:\n",
    "        return [], [], []\n",
    "    \n",
    "    random.shuffle(valid_pairs)\n",
    "    total = len(valid_pairs)\n",
    "    train_end = int(total * train_ratio)\n",
    "    val_end = train_end + int(total * val_ratio)\n",
    "    \n",
    "    train_pairs = valid_pairs[:train_end]\n",
    "    val_pairs = valid_pairs[train_end:val_end]\n",
    "    test_pairs = valid_pairs[val_end:]\n",
    "    \n",
    "    print(f\"Data splits:\")\n",
    "    print(f\"  Train: {len(train_pairs)} pairs\")\n",
    "    print(f\"  Validation: {len(val_pairs)} pairs\")\n",
    "    print(f\"  Test: {len(test_pairs)} pairs\")\n",
    "    \n",
    "    return train_pairs, val_pairs, test_pairs\n",
    "\n",
    "\n",
    "# CELL 5: BPE Tokenizer (From Scratch)\n",
    "\n",
    "class BPETokenizer:\n",
    "    def __init__(self, vocab_size: int = 10000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word_freqs = Counter()\n",
    "        self.vocab = {}\n",
    "        self.merges = []\n",
    "        self.special_tokens = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
    "        \n",
    "    def _get_stats(self, vocab):\n",
    "        pairs = defaultdict(int)\n",
    "        for word, freq in vocab.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pairs[symbols[i], symbols[i + 1]] += freq\n",
    "        return pairs\n",
    "    \n",
    "    def _merge_vocab(self, pair, vocab):\n",
    "        new_vocab = {}\n",
    "        bigram = re.escape(' '.join(pair))\n",
    "        p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "        \n",
    "        for word in vocab:\n",
    "            new_word = p.sub(''.join(pair), word)\n",
    "            new_vocab[new_word] = vocab[word]\n",
    "        return new_vocab\n",
    "    \n",
    "    def train(self, texts):\n",
    "        print(\"Training BPE tokenizer...\")\n",
    "        \n",
    "        for text in texts:\n",
    "            words = text.split()\n",
    "            for word in words:\n",
    "                self.word_freqs[word] += 1\n",
    "        \n",
    "        vocab = {}\n",
    "        for word, freq in self.word_freqs.items():\n",
    "            vocab[' '.join(list(word)) + ' </w>'] = freq\n",
    "        \n",
    "        for token in self.special_tokens:\n",
    "            vocab[token] = 1\n",
    "        \n",
    "        num_merges = self.vocab_size - len(self.special_tokens)\n",
    "        \n",
    "        for i in range(num_merges):\n",
    "            pairs = self._get_stats(vocab)\n",
    "            if not pairs:\n",
    "                break\n",
    "                \n",
    "            best_pair = max(pairs, key=pairs.get)\n",
    "            vocab = self._merge_vocab(best_pair, vocab)\n",
    "            self.merges.append(best_pair)\n",
    "            \n",
    "            if (i + 1) % 1000 == 0:\n",
    "                print(f\"Merged {i + 1}/{num_merges} pairs\")\n",
    "        \n",
    "        self.vocab = {}\n",
    "        for i, token in enumerate(self.special_tokens):\n",
    "            self.vocab[token] = i\n",
    "        \n",
    "        for word in vocab:\n",
    "            if word not in self.vocab:\n",
    "                self.vocab[word] = len(self.vocab)\n",
    "        \n",
    "        print(f\"BPE training completed. Vocabulary size: {len(self.vocab)}\")\n",
    "    \n",
    "    def encode(self, text):\n",
    "        # Simplified encoding for compatibility\n",
    "        tokens = []\n",
    "        words = text.split()\n",
    "        for word in words:\n",
    "            if word in self.vocab:\n",
    "                tokens.append(self.vocab[word])\n",
    "            else:\n",
    "                tokens.append(self.vocab['<unk>'])\n",
    "        return tokens\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        id_to_token = {v: k for k, v in self.vocab.items()}\n",
    "        tokens = []\n",
    "        for token_id in token_ids:\n",
    "            if token_id in id_to_token:\n",
    "                token = id_to_token[token_id]\n",
    "                if token not in self.special_tokens:\n",
    "                    tokens.append(token)\n",
    "        \n",
    "        text = ' '.join(tokens)\n",
    "        text = text.replace('</w>', ' ')\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return len(self.vocab)\n",
    "\n",
    "def create_tokenizers(train_pairs, src_vocab_size=8000, tgt_vocab_size=8000):\n",
    "    src_texts = [pair[0] for pair in train_pairs]\n",
    "    tgt_texts = [pair[1] for pair in train_pairs]\n",
    "    \n",
    "    src_tokenizer = BPETokenizer(vocab_size=src_vocab_size)\n",
    "    tgt_tokenizer = BPETokenizer(vocab_size=tgt_vocab_size)\n",
    "    \n",
    "    print(\"Training source (Urdu) tokenizer...\")\n",
    "    src_tokenizer.train(src_texts)\n",
    "    \n",
    "    print(\"Training target (Roman Urdu) tokenizer...\")\n",
    "    tgt_tokenizer.train(tgt_texts)\n",
    "    \n",
    "    return src_tokenizer, tgt_tokenizer\n",
    "\n",
    "\n",
    "# CELL 6: Model Architecture (WITH GPU/CPU FIX)\n",
    "\n",
    "class BiLSTMEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_dim: int, hidden_dim: int, \n",
    "                 num_layers: int = 2, dropout: float = 0.3):\n",
    "        super(BiLSTMEncoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, \n",
    "                           batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, lengths=None):\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        \n",
    "        if lengths is not None:\n",
    "            # FIX: Move lengths to CPU for pack_padded_sequence\n",
    "            lengths_cpu = lengths.cpu() if lengths.is_cuda else lengths\n",
    "            packed = nn.utils.rnn.pack_padded_sequence(\n",
    "                embedded, lengths_cpu, batch_first=True, enforce_sorted=False)\n",
    "            output, (hidden, cell) = self.lstm(packed)\n",
    "            output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "        else:\n",
    "            output, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        # Combine bidirectional hidden states\n",
    "        hidden_fwd = hidden[-2]\n",
    "        hidden_bwd = hidden[-1]\n",
    "        final_hidden = torch.cat([hidden_fwd, hidden_bwd], dim=1)\n",
    "        \n",
    "        cell_fwd = cell[-2]\n",
    "        cell_bwd = cell[-1]\n",
    "        final_cell = torch.cat([cell_fwd, cell_bwd], dim=1)\n",
    "        \n",
    "        return output, (final_hidden, final_cell)\n",
    "\n",
    "class AttentionMechanism(nn.Module):\n",
    "    def __init__(self, decoder_hidden_dim: int, encoder_hidden_dim: int):\n",
    "        super(AttentionMechanism, self).__init__()\n",
    "        # Use the actual encoder hidden dimension (1024 for bidirectional)\n",
    "        self.decoder_projection = nn.Linear(decoder_hidden_dim, encoder_hidden_dim)\n",
    "        self.encoder_projection = nn.Linear(encoder_hidden_dim, encoder_hidden_dim)\n",
    "        self.attention_projection = nn.Linear(encoder_hidden_dim, 1)\n",
    "\n",
    "    def forward(self, decoder_hidden, encoder_outputs, mask=None):\n",
    "        batch_size, seq_len, encoder_dim = encoder_outputs.size()\n",
    "\n",
    "        # decoder_hidden shape: (num_layers, batch_size, hidden_dim)\n",
    "        # We need: (batch_size, 1, hidden_dim) for attention\n",
    "        decoder_hidden_proj = decoder_hidden[-1].unsqueeze(1)  # Take last layer, add seq dim\n",
    "\n",
    "        # Project decoder hidden to encoder dimension\n",
    "        decoder_proj = self.decoder_projection(decoder_hidden_proj)  # (batch_size, 1, encoder_dim)\n",
    "\n",
    "        # Broadcast decoder projection to match encoder sequence length\n",
    "        decoder_proj = decoder_proj.repeat(1, seq_len, 1)  # (batch_size, seq_len, encoder_dim)\n",
    "\n",
    "        # Project encoder outputs to same dimension\n",
    "        encoder_proj = self.encoder_projection(encoder_outputs)  # (batch_size, seq_len, encoder_dim)\n",
    "\n",
    "        # Calculate attention scores\n",
    "        energy = torch.tanh(decoder_proj + encoder_proj)  # (batch_size, seq_len, encoder_dim)\n",
    "        attention_scores = self.attention_projection(energy).squeeze(2)  # (batch_size, seq_len)\n",
    "\n",
    "        if mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "        attention_weights = F.softmax(attention_scores, dim=1)  # (batch_size, seq_len)\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)  # (batch_size, 1, encoder_dim)\n",
    "\n",
    "        return context, attention_weights\n",
    "\n",
    "class LSTMDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_dim: int, hidden_dim: int, \n",
    "                 encoder_hidden_dim: int, num_layers: int = 4, dropout: float = 0.3):\n",
    "        super(LSTMDecoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        \n",
    "        self.hidden_projection = nn.Linear(encoder_hidden_dim, hidden_dim)\n",
    "        self.cell_projection = nn.Linear(encoder_hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Fix: Use the actual encoder hidden dimension (encoder_hidden_dim is already hidden_dim * 2)\n",
    "        self.attention = AttentionMechanism(hidden_dim, encoder_hidden_dim)\n",
    "        \n",
    "        # Fix: LSTM input should be embed_dim + encoder_hidden_dim (not *2)\n",
    "        self.lstm = nn.LSTM(embed_dim + encoder_hidden_dim, hidden_dim, \n",
    "                           num_layers, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        self.output_projection = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input_token, hidden, cell, encoder_outputs, mask=None):\n",
    "        embedded = self.dropout(self.embedding(input_token))\n",
    "        \n",
    "        # Fix: Pass the full hidden state to attention, not just the last layer\n",
    "        context, attention_weights = self.attention(hidden, encoder_outputs, mask)\n",
    "        \n",
    "        # Fix: Ensure context has the right shape for concatenation\n",
    "        lstm_input = torch.cat([embedded, context], dim=2)\n",
    "        output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
    "        output = self.output_projection(output)\n",
    "        \n",
    "        return output, hidden, cell, attention_weights\n",
    "\n",
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, src_vocab_size: int, tgt_vocab_size: int, \n",
    "                 embed_dim: int = 256, hidden_dim: int = 512, \n",
    "                 encoder_layers: int = 2, decoder_layers: int = 4,\n",
    "                 dropout: float = 0.3):\n",
    "        super(Seq2SeqModel, self).__init__()\n",
    "        \n",
    "        self.encoder = BiLSTMEncoder(\n",
    "            vocab_size=src_vocab_size,\n",
    "            embed_dim=embed_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_layers=encoder_layers,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.decoder = LSTMDecoder(\n",
    "            vocab_size=tgt_vocab_size,\n",
    "            embed_dim=embed_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            encoder_hidden_dim=hidden_dim * 2,  # This is correct - encoder outputs hidden_dim * 2\n",
    "            num_layers=decoder_layers,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.tgt_vocab_size = tgt_vocab_size\n",
    "        \n",
    "    def forward(self, src, tgt, src_lengths=None, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.size(0)\n",
    "        tgt_len = tgt.size(1)\n",
    "        \n",
    "        encoder_outputs, (hidden, cell) = self.encoder(src, src_lengths)\n",
    "        \n",
    "        if src_lengths is not None:\n",
    "            mask = torch.zeros(batch_size, src.size(1), device=src.device)\n",
    "            for i, length in enumerate(src_lengths):\n",
    "                mask[i, :length] = 1\n",
    "        else:\n",
    "            mask = torch.ones(batch_size, src.size(1), device=src.device)\n",
    "        \n",
    "        decoder_hidden = self.decoder.hidden_projection(hidden).unsqueeze(0)\n",
    "        decoder_cell = self.decoder.cell_projection(cell).unsqueeze(0)\n",
    "        \n",
    "        decoder_hidden = decoder_hidden.repeat(self.decoder.num_layers, 1, 1)\n",
    "        decoder_cell = decoder_cell.repeat(self.decoder.num_layers, 1, 1)\n",
    "        \n",
    "        outputs = torch.zeros(batch_size, tgt_len, self.tgt_vocab_size, device=src.device)\n",
    "        \n",
    "        input_token = tgt[:, 0].unsqueeze(1)\n",
    "        \n",
    "        for t in range(1, tgt_len):\n",
    "            output, decoder_hidden, decoder_cell, _ = self.decoder(\n",
    "                input_token, decoder_hidden, decoder_cell, encoder_outputs, mask)\n",
    "            \n",
    "            outputs[:, t] = output.squeeze(1)\n",
    "            \n",
    "            if random.random() < teacher_forcing_ratio:\n",
    "                input_token = tgt[:, t].unsqueeze(1)\n",
    "            else:\n",
    "                input_token = output.argmax(2)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "\n",
    "# CELL 7: Dataset and DataLoader\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, pairs, src_tokenizer, tgt_tokenizer):\n",
    "        self.pairs = pairs\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tgt_tokenizer = tgt_tokenizer\n",
    "        \n",
    "        self.tokenized_pairs = []\n",
    "        for src_text, tgt_text in pairs:\n",
    "            src_tokens = src_tokenizer.encode(src_text)\n",
    "            tgt_tokens = tgt_tokenizer.encode(f\"<sos> {tgt_text} <eos>\")\n",
    "            \n",
    "            if len(src_tokens) > 0 and len(tgt_tokens) > 0:\n",
    "                self.tokenized_pairs.append((src_tokens, tgt_tokens))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src_tokens, tgt_tokens = self.tokenized_pairs[idx]\n",
    "        return {\n",
    "            'src': torch.tensor(src_tokens, dtype=torch.long),\n",
    "            'tgt': torch.tensor(tgt_tokens, dtype=torch.long),\n",
    "            'src_len': len(src_tokens),\n",
    "            'tgt_len': len(tgt_tokens)\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_sequences = [item['src'] for item in batch]\n",
    "    tgt_sequences = [item['tgt'] for item in batch]\n",
    "    src_lengths = [item['src_len'] for item in batch]\n",
    "    tgt_lengths = [item['tgt_len'] for item in batch]\n",
    "    \n",
    "    src_padded = pad_sequence(src_sequences, batch_first=True, padding_value=0)\n",
    "    tgt_padded = pad_sequence(tgt_sequences, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return {\n",
    "        'src': src_padded,\n",
    "        'tgt': tgt_padded,\n",
    "        'src_lengths': torch.tensor(src_lengths, dtype=torch.long),\n",
    "        'tgt_lengths': torch.tensor(tgt_lengths, dtype=torch.long)\n",
    "    }\n",
    "\n",
    "def create_data_loaders(train_pairs, val_pairs, test_pairs, src_tokenizer, tgt_tokenizer, \n",
    "                       batch_size=32, num_workers=0):\n",
    "    train_dataset = TranslationDataset(train_pairs, src_tokenizer, tgt_tokenizer)\n",
    "    val_dataset = TranslationDataset(val_pairs, src_tokenizer, tgt_tokenizer)\n",
    "    test_dataset = TranslationDataset(test_pairs, src_tokenizer, tgt_tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, \n",
    "        collate_fn=collate_fn, num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, \n",
    "        collate_fn=collate_fn, num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=batch_size, shuffle=False, \n",
    "        collate_fn=collate_fn, num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    print(f\"Data loaders created:\")\n",
    "    print(f\"  Train: {len(train_loader)} batches\")\n",
    "    print(f\"  Validation: {len(val_loader)} batches\")\n",
    "    print(f\"  Test: {len(test_loader)} batches\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "# CELL 8: Training Class\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import time\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, val_loader, src_tokenizer, tgt_tokenizer, \n",
    "                 lr=1e-3, device='cuda', checkpoint_system=None):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tgt_tokenizer = tgt_tokenizer\n",
    "        self.device = device\n",
    "        self.checkpoint_system = checkpoint_system\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer, mode='min', patience=3, factor=0.5)\n",
    "        \n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_perplexities = []\n",
    "        self.val_perplexities = []\n",
    "        \n",
    "    def train_epoch(self, epoch, teacher_forcing_ratio=0.5):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        total_tokens = 0\n",
    "        \n",
    "        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch}')\n",
    "        for batch_idx, batch in enumerate(pbar):\n",
    "            src = batch['src'].to(self.device)\n",
    "            tgt = batch['tgt'].to(self.device)\n",
    "            src_lengths = batch['src_lengths'].to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            outputs = self.model(src, tgt, src_lengths, teacher_forcing_ratio)\n",
    "            \n",
    "            outputs = outputs[:, 1:].contiguous().view(-1, outputs.size(-1))\n",
    "            targets = tgt[:, 1:].contiguous().view(-1)\n",
    "            \n",
    "            loss = self.criterion(outputs, targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_tokens += targets.ne(0).sum().item()\n",
    "            \n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        avg_loss = total_loss / len(self.train_loader)\n",
    "        perplexity = np.exp(total_loss * len(self.train_loader) / total_tokens)\n",
    "        \n",
    "        return avg_loss, perplexity\n",
    "    \n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        total_tokens = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(self.val_loader, desc='Validation'):\n",
    "                src = batch['src'].to(self.device)\n",
    "                tgt = batch['tgt'].to(self.device)\n",
    "                src_lengths = batch['src_lengths'].to(self.device)\n",
    "                \n",
    "                outputs = self.model(src, tgt, src_lengths, teacher_forcing_ratio=0)\n",
    "                \n",
    "                outputs = outputs[:, 1:].contiguous().view(-1, outputs.size(-1))\n",
    "                targets = tgt[:, 1:].contiguous().view(-1)\n",
    "                \n",
    "                loss = self.criterion(outputs, targets)\n",
    "                total_loss += loss.item()\n",
    "                total_tokens += targets.ne(0).sum().item()\n",
    "        \n",
    "        avg_loss = total_loss / len(self.val_loader)\n",
    "        perplexity = np.exp(total_loss * len(self.val_loader) / total_tokens)\n",
    "        \n",
    "        return avg_loss, perplexity\n",
    "    \n",
    "    def train(self, num_epochs, save_path='best_model.pth'):\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        max_patience = 5\n",
    "        \n",
    "        print(f\"Starting training for {num_epochs} epochs...\")\n",
    "        print(f\"Device: {self.device}\")\n",
    "        print(f\"Model parameters: {sum(p.numel() for p in self.model.parameters()):,}\")\n",
    "        \n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            train_loss, train_perplexity = self.train_epoch(epoch)\n",
    "            val_loss, val_perplexity = self.validate()\n",
    "            \n",
    "            self.scheduler.step(val_loss)\n",
    "            \n",
    "            self.train_losses.append(train_loss)\n",
    "            self.val_losses.append(val_loss)\n",
    "            self.train_perplexities.append(train_perplexity)\n",
    "            self.val_perplexities.append(val_perplexity)\n",
    "            \n",
    "            epoch_time = time.time() - start_time\n",
    "            \n",
    "            print(f'Epoch {epoch}/{num_epochs}:')\n",
    "            print(f'  Train Loss: {train_loss:.4f}, Train Perplexity: {train_perplexity:.4f}')\n",
    "            print(f'  Val Loss: {val_loss:.4f}, Val Perplexity: {val_perplexity:.4f}')\n",
    "            print(f'  Time: {epoch_time:.2f}s, LR: {self.optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "            \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'train_losses': self.train_losses,\n",
    "                    'val_losses': self.val_losses,\n",
    "                    'train_perplexities': self.train_perplexities,\n",
    "                    'val_perplexities': self.val_perplexities,\n",
    "                    'best_val_loss': best_val_loss\n",
    "                }, save_path)\n",
    "                print(f'  New best model saved! Val Loss: {val_loss:.4f}')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Save checkpoint every 2 epochs if checkpoint system is available\n",
    "            if self.checkpoint_system and epoch % 2 == 0:\n",
    "                self.checkpoint_system.save_checkpoint(\n",
    "                    epoch, self.model, self.optimizer, \n",
    "                    self.train_losses, self.val_losses,\n",
    "                    self.train_perplexities, self.val_perplexities,\n",
    "                    best_val_loss, {}\n",
    "                )\n",
    "                \n",
    "            if patience_counter >= max_patience:\n",
    "                print(f'Early stopping after {epoch} epochs')\n",
    "                break\n",
    "                \n",
    "            print('-' * 60)\n",
    "        \n",
    "        print('Training completed!')\n",
    "        return self.train_losses, self.val_losses\n",
    "    \n",
    "    def plot_training_curves(self, save_path='training_curves.png'):\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        ax1.plot(self.train_losses, label='Train Loss', color='blue')\n",
    "        ax1.plot(self.val_losses, label='Validation Loss', color='red')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_title('Training and Validation Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        ax2.plot(self.train_perplexities, label='Train Perplexity', color='blue')\n",
    "        ax2.plot(self.val_perplexities, label='Validation Perplexity', color='red')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Perplexity')\n",
    "        ax2.set_title('Training and Validation Perplexity')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "class Evaluator:\n",
    "    \n",
    "    def __init__(self, model, src_tokenizer, tgt_tokenizer, device='cuda'):\n",
    "        self.model = model\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tgt_tokenizer = tgt_tokenizer\n",
    "        self.device = device\n",
    "        \n",
    "    def calculate_accuracy(self, predictions, targets):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for pred, target in zip(predictions, targets):\n",
    "            # Remove padding tokens (0) for comparison\n",
    "            pred_clean = [token for token in pred if token != 0]\n",
    "            target_clean = [token for token in target if token != 0]\n",
    "            \n",
    "            # Calculate word-level accuracy\n",
    "            min_len = min(len(pred_clean), len(target_clean))\n",
    "            if min_len > 0:\n",
    "                correct += sum(1 for p, t in zip(pred_clean[:min_len], target_clean[:min_len]) if p == t)\n",
    "                total += len(target_clean)\n",
    "        \n",
    "        accuracy = (correct / total * 100) if total > 0 else 0\n",
    "        return accuracy, correct, total\n",
    "    \n",
    "    def calculate_bleu_score(self, predictions, targets):\n",
    "        try:\n",
    "            from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "            \n",
    "            bleu_scores = []\n",
    "            smoothie = SmoothingFunction().method4\n",
    "            \n",
    "            for pred, target in zip(predictions, targets):\n",
    "                # Convert token IDs to words\n",
    "                pred_words = [self.tgt_tokenizer.idx_to_word.get(token, '<UNK>') for token in pred if token != 0]\n",
    "                target_words = [self.tgt_tokenizer.idx_to_word.get(token, '<UNK>') for token in target if token != 0]\n",
    "                \n",
    "                if len(pred_words) > 0 and len(target_words) > 0:\n",
    "                    bleu = sentence_bleu([target_words], pred_words, smoothing_function=smoothie)\n",
    "                    bleu_scores.append(bleu)\n",
    "            \n",
    "            return sum(bleu_scores) / len(bleu_scores) * 100 if bleu_scores else 0\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    def evaluate_dataset(self, data_loader, dataset_name=\"Dataset\"):\n",
    "        self.model.eval()\n",
    "        all_predictions = []\n",
    "        all_targets = []\n",
    "        total_loss = 0\n",
    "        total_tokens = 0\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "        \n",
    "        print(f\"\\\\nüìä Evaluating {dataset_name}...\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(data_loader, desc=f\"Evaluating {dataset_name}\"):\n",
    "                src = batch['src'].to(self.device)\n",
    "                tgt = batch['tgt'].to(self.device)\n",
    "                src_lengths = batch['src_lengths'].to(self.device)\n",
    "                \n",
    "                # Get predictions\n",
    "                outputs = self.model(src, tgt, src_lengths, teacher_forcing_ratio=0.0)\n",
    "                \n",
    "                # Calculate loss\n",
    "                outputs_flat = outputs[:, 1:].contiguous().view(-1, outputs.size(-1))\n",
    "                targets_flat = tgt[:, 1:].contiguous().view(-1)\n",
    "                loss = criterion(outputs_flat, targets_flat)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                total_tokens += (targets_flat != 0).sum().item()\n",
    "                \n",
    "                # Get predictions\n",
    "                predictions = outputs.argmax(dim=-1)\n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_targets.extend(tgt.cpu().numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        perplexity = np.exp(avg_loss)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy, correct, total = self.calculate_accuracy(all_predictions, all_targets)\n",
    "        \n",
    "        # Calculate BLEU score\n",
    "        bleu_score = self.calculate_bleu_score(all_predictions, all_targets)\n",
    "        \n",
    "        print(f\"\\\\nüìà {dataset_name} Results:\")\n",
    "        print(f\"  Loss: {avg_loss:.4f}\")\n",
    "        print(f\"  Perplexity: {perplexity:.4f}\")\n",
    "        print(f\"  Accuracy: {accuracy:.2f}% ({correct}/{total} words)\")\n",
    "        print(f\"  BLEU Score: {bleu_score:.2f}\")\n",
    "        \n",
    "        return {\n",
    "            'loss': avg_loss,\n",
    "            'perplexity': perplexity,\n",
    "            'accuracy': accuracy,\n",
    "            'correct_words': correct,\n",
    "            'total_words': total,\n",
    "            'bleu_score': bleu_score\n",
    "        }\n",
    "\n",
    "\n",
    "# CELL 9: Main Training Execution\n",
    "\n",
    "# Main training execution\n",
    "config = {\n",
    "    'seed': 42,\n",
    "    'embed_dim': 256,\n",
    "    'hidden_dim': 512,\n",
    "    'encoder_layers': 2,\n",
    "    'decoder_layers': 4,\n",
    "    'dropout': 0.3,\n",
    "    'learning_rate': 1e-3,\n",
    "    'batch_size': 32,\n",
    "    'src_vocab_size': 8000,\n",
    "    'tgt_vocab_size': 8000,\n",
    "    'num_epochs': 10,\n",
    "    'use_minimal_dataset': False,   # Set to True for quick testing with 2000 words\n",
    "    'minimal_dataset_size': 1000   # Limit to 2000 words for time-saving\n",
    "}\n",
    "\n",
    "print(\"Starting Urdu to Roman Urdu NMT Training...\")\n",
    "print(f\"Configuration: {config}\")\n",
    "\n",
    "# Dataset limitation notice\n",
    "if config['use_minimal_dataset']:\n",
    "    print(f\"\\n‚ö†Ô∏è DATASET LIMITATION: Using only {config['minimal_dataset_size']} words for quick testing\")\n",
    "    print(\"üí° This saves time but may not reflect full model performance\")\n",
    "    print(\"üí° If results look good, change 'use_minimal_dataset': False for full training\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Using full dataset for complete training\")\n",
    "\n",
    "# 1. Load and preprocess data\n",
    "print(\"\\n1. Loading and preprocessing data...\")\n",
    "\n",
    "# Find correct dataset path (avoid __MACOSX)\n",
    "dataset_path = None\n",
    "if os.path.exists('/content/dataset_extracted'):\n",
    "    for root, dirs, files in os.walk('/content/dataset_extracted'):\n",
    "        if '__MACOSX' in root:\n",
    "            continue\n",
    "        if any(poet in dirs for poet in ['mirza-ghalib', 'ahmad-faraz', 'allama-iqbal']):\n",
    "            dataset_path = root\n",
    "            print(f\"‚úÖ Found dataset at: {dataset_path}\")\n",
    "            break\n",
    "\n",
    "if dataset_path:\n",
    "    if config['use_minimal_dataset']:\n",
    "        print(f\"üöÄ Using minimal dataset for quick testing ({config['minimal_dataset_size']} pairs)...\")\n",
    "        urdu_texts, roman_texts = create_minimal_dataset(dataset_path, config['minimal_dataset_size'])\n",
    "    else:\n",
    "        urdu_texts, roman_texts = load_dataset(dataset_path)\n",
    "    \n",
    "    train_pairs, val_pairs, test_pairs = clean_and_split_data(\n",
    "        urdu_texts, roman_texts,\n",
    "        train_ratio=0.5, val_ratio=0.25, test_ratio=0.25\n",
    "    )\n",
    "    \n",
    "    if len(train_pairs) == 0:\n",
    "        print(\"‚ùå No training data available!\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Ready to proceed with {len(train_pairs)} training pairs\")\n",
    "\n",
    "        # 2. Create tokenizers with checkpoint system\n",
    "        print(\"\\n2. Creating tokenizers with checkpoint system...\")\n",
    "        \n",
    "        # Initialize checkpoint systems\n",
    "        tokenizer_checkpoint = TokenizerCheckpoint()\n",
    "        model_checkpoint = ModelCheckpoint()\n",
    "        \n",
    "        # Mount Google Drive for persistent storage\n",
    "        print(\"üîó Mounting Google Drive for persistent storage...\")\n",
    "        drive_mounted = mount_google_drive()\n",
    "        \n",
    "        # Create tokenizers with checkpoint support\n",
    "        src_tokenizer, tgt_tokenizer = create_tokenizers_with_checkpoint(\n",
    "            train_pairs, config, tokenizer_checkpoint\n",
    "        )\n",
    "\n",
    "        # 3. Create data loaders\n",
    "        print(\"\\n3. Creating data loaders...\")\n",
    "        train_loader, val_loader, test_loader = create_data_loaders(\n",
    "            train_pairs, val_pairs, test_pairs,\n",
    "            src_tokenizer, tgt_tokenizer,\n",
    "            batch_size=config['batch_size']\n",
    "        )\n",
    "\n",
    "        # 4. Create model\n",
    "        print(\"\\n4. Creating model...\")\n",
    "        model = Seq2SeqModel(\n",
    "            src_vocab_size=src_tokenizer.get_vocab_size(),\n",
    "            tgt_vocab_size=tgt_tokenizer.get_vocab_size(),\n",
    "            embed_dim=config['embed_dim'],\n",
    "            hidden_dim=config['hidden_dim'],\n",
    "            encoder_layers=config['encoder_layers'],\n",
    "            decoder_layers=config['decoder_layers'],\n",
    "            dropout=config['dropout']\n",
    "        )\n",
    "\n",
    "        print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "        # 5. Train model\n",
    "        print(\"\\n5. Starting training...\")\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            src_tokenizer=src_tokenizer,\n",
    "            tgt_tokenizer=tgt_tokenizer,\n",
    "            lr=config['learning_rate'],\n",
    "            device=device,\n",
    "            checkpoint_system=model_checkpoint\n",
    "        )\n",
    "\n",
    "        train_losses, val_losses = trainer.train(\n",
    "            num_epochs=config['num_epochs'],\n",
    "            save_path='best_model.pth'\n",
    "        )\n",
    "\n",
    "        # 6. Evaluate model on validation and test sets\n",
    "        print(\"\\n6. Evaluating model performance...\")\n",
    "        evaluator = Evaluator(model, src_tokenizer, tgt_tokenizer, device)\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        val_results = evaluator.evaluate_dataset(val_loader, \"Validation Set\")\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        test_results = evaluator.evaluate_dataset(test_loader, \"Test Set\")\n",
    "        \n",
    "        # 7. Plot training curves\n",
    "        trainer.plot_training_curves('training_curves.png')\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # 8. Show comprehensive results summary\n",
    "        print(\"\\nüìä COMPREHENSIVE RESULTS SUMMARY:\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"üìà VALIDATION SET PERFORMANCE:\")\n",
    "        print(f\"  ‚Ä¢ Accuracy: {val_results['accuracy']:.2f}%\")\n",
    "        print(f\"  ‚Ä¢ BLEU Score: {val_results['bleu_score']:.2f}\")\n",
    "        print(f\"  ‚Ä¢ Perplexity: {val_results['perplexity']:.4f}\")\n",
    "        print(f\"  ‚Ä¢ Loss: {val_results['loss']:.4f}\")\n",
    "        print(f\"  ‚Ä¢ Correct Words: {val_results['correct_words']}/{val_results['total_words']}\")\n",
    "        \n",
    "        print(f\"\\nüìà TEST SET PERFORMANCE:\")\n",
    "        print(f\"  ‚Ä¢ Accuracy: {test_results['accuracy']:.2f}%\")\n",
    "        print(f\"  ‚Ä¢ BLEU Score: {test_results['bleu_score']:.2f}\")\n",
    "        print(f\"  ‚Ä¢ Perplexity: {test_results['perplexity']:.4f}\")\n",
    "        print(f\"  ‚Ä¢ Loss: {test_results['loss']:.4f}\")\n",
    "        print(f\"  ‚Ä¢ Correct Words: {test_results['correct_words']}/{test_results['total_words']}\")\n",
    "        \n",
    "        # Performance assessment\n",
    "        print(f\"\\nüéØ PERFORMANCE ASSESSMENT:\")\n",
    "        if val_results['accuracy'] > 80:\n",
    "            print(\"‚úÖ EXCELLENT: Accuracy > 80% - Model is performing very well!\")\n",
    "        elif val_results['accuracy'] > 60:\n",
    "            print(\"‚úÖ GOOD: Accuracy > 60% - Model is performing well!\")\n",
    "        elif val_results['accuracy'] > 40:\n",
    "            print(\"‚ö†Ô∏è FAIR: Accuracy > 40% - Model needs improvement\")\n",
    "        else:\n",
    "            print(\"‚ùå POOR: Accuracy < 40% - Model needs significant improvement\")\n",
    "        \n",
    "        if config['use_minimal_dataset']:\n",
    "            print(f\"\\nüí° DATASET LIMIT: Using {config['minimal_dataset_size']} words for quick testing\")\n",
    "            print(\"üí° If results look good, set 'use_minimal_dataset': False for full training\")\n",
    "        \n",
    "        # Show time-saving summary\n",
    "        print(\"\\nüöÄ TIME-SAVING FEATURES USED:\")\n",
    "        print(\"=\" * 40)\n",
    "        print(\"‚úÖ Tokenizer Checkpoint System - Saves 15+ minutes on next run\")\n",
    "        print(\"‚úÖ Model Checkpoint System - Saves training progress every 2 epochs\")\n",
    "        print(\"‚úÖ Google Drive Mounting - Persistent storage across sessions\")\n",
    "        if config['use_minimal_dataset']:\n",
    "            print(\"‚úÖ Minimal Dataset Testing - Quick validation with subset\")\n",
    "        print(\"\\nüí° NEXT RUN: Tokenizers will load instantly from checkpoint!\")\n",
    "        print(\"üí° TRAINING: Can resume from any saved checkpoint if interrupted!\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Could not find dataset!\")\n",
    "\n",
    "\n",
    "print(\"=== COMPLETE ERROR-FREE SOLUTION READY ===\")\n",
    "print(\"Copy each cell block (between triple quotes) into separate Colab cells\")\n",
    "print(\"Run them in order for guaranteed success!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b3f375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53802fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 10: Experiment System with Different Hyperparameters\n",
    "\n",
    "# =============================================================================\n",
    "# EXPERIMENT SYSTEM - TRAIN MULTIPLE MODELS WITH DIFFERENT HYPERPARAMETERS\n",
    "# =============================================================================\n",
    "\n",
    "class ExperimentRunner:\n",
    "    def __init__(self):\n",
    "        self.results = []\n",
    "        self.best_model = None\n",
    "        self.best_score = 0\n",
    "        \n",
    "    def run_single_experiment(self, config, experiment_num):\n",
    "        \"\"\"Run a single experiment with given configuration\"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"üöÄ RUNNING Experiment {experiment_num}: {config['name']}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Set random seed for reproducibility\n",
    "        torch.manual_seed(config['seed'])\n",
    "        np.random.seed(config['seed'])\n",
    "        random.seed(config['seed'])\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(config['seed'])\n",
    "        \n",
    "        try:\n",
    "            # 1. Load data (same for all experiments)\n",
    "            print(f\"\\n1. Loading data for {config['name']}...\")\n",
    "            \n",
    "            # Find correct dataset path\n",
    "            dataset_path = None\n",
    "            if os.path.exists('/content/dataset_extracted'):\n",
    "                for root, dirs, files in os.walk('/content/dataset_extracted'):\n",
    "                    if '__MACOSX' in root:\n",
    "                        continue\n",
    "                    if any(poet in dirs for poet in ['mirza-ghalib', 'ahmad-faraz', 'allama-iqbal']):\n",
    "                        dataset_path = root\n",
    "                        print(f\"‚úÖ Found dataset at: {dataset_path}\")\n",
    "                        break\n",
    "            \n",
    "            if not dataset_path:\n",
    "                print(\"‚ùå Dataset not found!\")\n",
    "                return None\n",
    "            \n",
    "            # Load dataset\n",
    "            if config.get('use_minimal_dataset', False):\n",
    "                print(f\"üöÄ Using minimal dataset ({config.get('minimal_dataset_size', 1000)} pairs)...\")\n",
    "                urdu_texts, roman_texts = create_minimal_dataset(dataset_path, config.get('minimal_dataset_size', 1000))\n",
    "            else:\n",
    "                urdu_texts, roman_texts = load_dataset(dataset_path)\n",
    "            \n",
    "            # Clean and split data (same splits for all experiments)\n",
    "            train_pairs, val_pairs, test_pairs = clean_and_split_data(\n",
    "                urdu_texts, roman_texts,\n",
    "                train_ratio=0.5, val_ratio=0.25, test_ratio=0.25\n",
    "            )\n",
    "            \n",
    "            if len(train_pairs) == 0:\n",
    "                print(\"‚ùå No training data available!\")\n",
    "                return None\n",
    "            \n",
    "            print(f\"‚úÖ Data loaded: {len(train_pairs)} train, {len(val_pairs)} val, {len(test_pairs)} test pairs\")\n",
    "            \n",
    "            # 2. Create tokenizers with checkpoint system\n",
    "            print(f\"\\n2. Creating tokenizers for {config['name']}...\")\n",
    "            \n",
    "            # Initialize checkpoint systems for this experiment\n",
    "            exp_dir = f\"exp_{experiment_num}\"\n",
    "            tokenizer_checkpoint = TokenizerCheckpoint(f\"/content/tokenizer_checkpoints_{exp_dir}\")\n",
    "            model_checkpoint = ModelCheckpoint(f\"/content/model_checkpoints_{exp_dir}\")\n",
    "            \n",
    "            # Create tokenizers\n",
    "            src_tokenizer, tgt_tokenizer = create_tokenizers_with_checkpoint(\n",
    "                train_pairs, config, tokenizer_checkpoint\n",
    "            )\n",
    "            \n",
    "            # 3. Create data loaders\n",
    "            print(f\"\\n3. Creating data loaders for {config['name']}...\")\n",
    "            train_loader, val_loader, test_loader = create_data_loaders(\n",
    "                train_pairs, val_pairs, test_pairs,\n",
    "                src_tokenizer, tgt_tokenizer,\n",
    "                batch_size=config['batch_size']\n",
    "            )\n",
    "            \n",
    "            # 4. Create model\n",
    "            print(f\"\\n4. Creating model for {config['name']}...\")\n",
    "            model = Seq2SeqModel(\n",
    "                src_vocab_size=src_tokenizer.get_vocab_size(),\n",
    "                tgt_vocab_size=tgt_tokenizer.get_vocab_size(),\n",
    "                embed_dim=config['embed_dim'],\n",
    "                hidden_dim=config['hidden_dim'],\n",
    "                encoder_layers=config['encoder_layers'],\n",
    "                decoder_layers=config['decoder_layers'],\n",
    "                dropout=config['dropout']\n",
    "            )\n",
    "            \n",
    "            print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "            \n",
    "            # 5. Train model\n",
    "            print(f\"\\n5. Training {config['name']}...\")\n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader,\n",
    "                src_tokenizer=src_tokenizer,\n",
    "                tgt_tokenizer=tgt_tokenizer,\n",
    "                lr=config['learning_rate'],\n",
    "                device=device,\n",
    "                checkpoint_system=model_checkpoint\n",
    "            )\n",
    "            \n",
    "            train_losses, val_losses = trainer.train(\n",
    "                num_epochs=config['num_epochs'],\n",
    "                save_path=f'best_model_exp_{experiment_num}.pth'\n",
    "            )\n",
    "            \n",
    "            # 6. Evaluate model\n",
    "            print(f\"\\n6. Evaluating {config['name']}...\")\n",
    "            evaluator = Evaluator(model, src_tokenizer, tgt_tokenizer, device)\n",
    "            \n",
    "            # Evaluate on validation set\n",
    "            val_results = evaluator.evaluate_dataset(val_loader, f\"Validation Set - {config['name']}\")\n",
    "            \n",
    "            # Evaluate on test set\n",
    "            test_results = evaluator.evaluate_dataset(test_loader, f\"Test Set - {config['name']}\")\n",
    "            \n",
    "            # 7. Calculate additional metrics\n",
    "            print(f\"\\n7. Calculating additional metrics for {config['name']}...\")\n",
    "            \n",
    "            # Calculate Character Error Rate (CER)\n",
    "            val_cer = self.calculate_cer(model, val_loader, src_tokenizer, tgt_tokenizer, device)\n",
    "            test_cer = self.calculate_cer(model, test_loader, src_tokenizer, tgt_tokenizer, device)\n",
    "            \n",
    "            # Get qualitative examples\n",
    "            qualitative_examples = self.get_qualitative_examples(\n",
    "                model, val_loader, src_tokenizer, tgt_tokenizer, device, num_examples=3\n",
    "            )\n",
    "            \n",
    "            # Store results\n",
    "            result = {\n",
    "                'experiment_num': experiment_num,\n",
    "                'config': config,\n",
    "                'val_results': val_results,\n",
    "                'test_results': test_results,\n",
    "                'val_cer': val_cer,\n",
    "                'test_cer': test_cer,\n",
    "                'qualitative_examples': qualitative_examples,\n",
    "                'model_path': f'best_model_exp_{experiment_num}.pth',\n",
    "                'tokenizer_path': f'/content/tokenizer_checkpoints_{exp_dir}'\n",
    "            }\n",
    "            \n",
    "            # Update best model\n",
    "            if val_results['accuracy'] > self.best_score:\n",
    "                self.best_score = val_results['accuracy']\n",
    "                self.best_model = result\n",
    "                print(f\"üèÜ New best model! Accuracy: {val_results['accuracy']:.2f}%\")\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in experiment {experiment_num}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def calculate_cer(self, model, data_loader, src_tokenizer, tgt_tokenizer, device, num_samples=100):\n",
    "        \"\"\"Calculate Character Error Rate\"\"\"\n",
    "        model.eval()\n",
    "        total_cer = 0\n",
    "        samples_processed = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in data_loader:\n",
    "                if samples_processed >= num_samples:\n",
    "                    break\n",
    "                    \n",
    "                src = batch['src'].to(device)\n",
    "                tgt = batch['tgt'].to(device)\n",
    "                src_lengths = batch['src_lengths'].to(device)\n",
    "                \n",
    "                # Get predictions\n",
    "                outputs = model(src, tgt, src_lengths, teacher_forcing_ratio=0.0)\n",
    "                predictions = outputs.argmax(dim=-1)\n",
    "                \n",
    "                # Convert to text and calculate CER\n",
    "                for j in range(src.size(0)):\n",
    "                    if samples_processed >= num_samples:\n",
    "                        break\n",
    "                        \n",
    "                    pred_tokens = predictions[j].cpu().numpy()\n",
    "                    target_tokens = tgt[j].cpu().numpy()\n",
    "                    \n",
    "                    # Remove padding and convert to text\n",
    "                    pred_clean = [token for token in pred_tokens if token != 0]\n",
    "                    target_clean = [token for token in target_tokens if token != 0]\n",
    "                    \n",
    "                    # Convert to text using decode method\n",
    "                    pred_text = tgt_tokenizer.decode(pred_clean)\n",
    "                    target_text = tgt_tokenizer.decode(target_clean)\n",
    "                    \n",
    "                    # Calculate edit distance\n",
    "                    if len(target_text) > 0:\n",
    "                        cer = self.edit_distance(pred_text, target_text) / len(target_text)\n",
    "                        total_cer += cer\n",
    "                        samples_processed += 1\n",
    "        \n",
    "        return total_cer / samples_processed if samples_processed > 0 else 0\n",
    "    \n",
    "    def edit_distance(self, s1, s2):\n",
    "        \"\"\"Calculate Levenshtein distance\"\"\"\n",
    "        if len(s1) < len(s2):\n",
    "            return self.edit_distance(s2, s1)\n",
    "        \n",
    "        if len(s2) == 0:\n",
    "            return len(s1)\n",
    "        \n",
    "        previous_row = list(range(len(s2) + 1))\n",
    "        for i, c1 in enumerate(s1):\n",
    "            current_row = [i + 1]\n",
    "            for j, c2 in enumerate(s2):\n",
    "                insertions = previous_row[j + 1] + 1\n",
    "                deletions = current_row[j] + 1\n",
    "                substitutions = previous_row[j] + (c1 != c2)\n",
    "                current_row.append(min(insertions, deletions, substitutions))\n",
    "            previous_row = current_row\n",
    "        \n",
    "        return previous_row[-1]\n",
    "    \n",
    "    def get_qualitative_examples(self, model, data_loader, src_tokenizer, tgt_tokenizer, device, num_examples=3):\n",
    "        \"\"\"Get qualitative translation examples\"\"\"\n",
    "        model.eval()\n",
    "        examples = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(data_loader):\n",
    "                if len(examples) >= num_examples:\n",
    "                    break\n",
    "                    \n",
    "                src = batch['src'].to(device)\n",
    "                tgt = batch['tgt'].to(device)\n",
    "                src_lengths = batch['src_lengths'].to(device)\n",
    "                \n",
    "                # Get predictions\n",
    "                outputs = model(src, tgt, src_lengths, teacher_forcing_ratio=0.0)\n",
    "                predictions = outputs.argmax(dim=-1)\n",
    "                \n",
    "                for j in range(min(num_examples - len(examples), src.size(0))):\n",
    "                    # Source text\n",
    "                    src_tokens = src[j].cpu().numpy()\n",
    "                    src_clean = [token for token in src_tokens if token != 0]\n",
    "                    src_text = src_tokenizer.decode(src_clean)\n",
    "                    \n",
    "                    # Target text\n",
    "                    target_tokens = tgt[j].cpu().numpy()\n",
    "                    target_clean = [token for token in target_tokens if token != 0]\n",
    "                    target_text = tgt_tokenizer.decode(target_clean)\n",
    "                    \n",
    "                    # Predicted text\n",
    "                    pred_tokens = predictions[j].cpu().numpy()\n",
    "                    pred_clean = [token for token in pred_tokens if token != 0]\n",
    "                    pred_text = tgt_tokenizer.decode(pred_clean)\n",
    "                    \n",
    "                    examples.append({\n",
    "                        'source': src_text,\n",
    "                        'target': target_text,\n",
    "                        'prediction': pred_text\n",
    "                    })\n",
    "        \n",
    "        return examples\n",
    "\n",
    "# Define experiment configurations\n",
    "experiment_configs = [\n",
    "    {\n",
    "        'name': 'Experiment 1: Small Model',\n",
    "        'seed': 42,\n",
    "        'embed_dim': 128,\n",
    "        'hidden_dim': 256,\n",
    "        'encoder_layers': 1,\n",
    "        'decoder_layers': 2,\n",
    "        'dropout': 0.1,\n",
    "        'learning_rate': 1e-3,\n",
    "        'batch_size': 32,\n",
    "        'src_vocab_size': 8000,\n",
    "        'tgt_vocab_size': 8000,\n",
    "        'num_epochs': 10,\n",
    "        'use_minimal_dataset': True,\n",
    "        'minimal_dataset_size': 2000\n",
    "    },\n",
    "    {\n",
    "        'name': 'Experiment 2: Medium Model',\n",
    "        'seed': 42,\n",
    "        'embed_dim': 256,\n",
    "        'hidden_dim': 512,\n",
    "        'encoder_layers': 2,\n",
    "        'decoder_layers': 3,\n",
    "        'dropout': 0.3,\n",
    "        'learning_rate': 5e-4,\n",
    "        'batch_size': 64,\n",
    "        'src_vocab_size': 8000,\n",
    "        'tgt_vocab_size': 8000,\n",
    "        'num_epochs': 10,\n",
    "        'use_minimal_dataset': True,\n",
    "        'minimal_dataset_size': 2000\n",
    "    },\n",
    "    {\n",
    "        'name': 'Experiment 3: Large Model',\n",
    "        'seed': 42,\n",
    "        'embed_dim': 512,\n",
    "        'hidden_dim': 512,\n",
    "        'encoder_layers': 3,\n",
    "        'decoder_layers': 4,\n",
    "        'dropout': 0.5,\n",
    "        'learning_rate': 1e-4,\n",
    "        'batch_size': 128,\n",
    "        'src_vocab_size': 8000,\n",
    "        'tgt_vocab_size': 8000,\n",
    "        'num_epochs': 10,\n",
    "        'use_minimal_dataset': True,\n",
    "        'minimal_dataset_size': 2000\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cea211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 12: Run All Experiments\n",
    "\n",
    "print(\"\\nüöÄ STARTING EXPERIMENTATION PHASE\")\n",
    "print(\"=\"*80)\n",
    "print(\"Running 3 experiments with different parameter configurations...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize experiment runner\n",
    "experiment_runner = ExperimentRunner()\n",
    "\n",
    "# Run all experiments\n",
    "results = []\n",
    "for i, config in enumerate(experiment_configs, 1):\n",
    "    result = experiment_runner.run_single_experiment(config, i)\n",
    "    if result:\n",
    "        results.append(result)\n",
    "\n",
    "# Generate comprehensive report\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä EXPERIMENTATION RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if results:\n",
    "    # Create results table\n",
    "    print(f\"\\n{'Experiment':<20} {'Val Acc':<10} {'Test Acc':<10} {'Val BLEU':<10} {'Test BLEU':<10} {'Val CER':<10} {'Test CER':<10}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    for result in results:\n",
    "        config = result['config']\n",
    "        val_results = result['val_results']\n",
    "        test_results = result['test_results']\n",
    "        \n",
    "        print(f\"{config['name']:<20} \"\n",
    "              f\"{val_results['accuracy']:<10.2f} \"\n",
    "              f\"{test_results['accuracy']:<10.2f} \"\n",
    "              f\"{val_results['bleu_score']:<10.2f} \"\n",
    "              f\"{test_results['bleu_score']:<10.2f} \"\n",
    "              f\"{result['val_cer']:<10.4f} \"\n",
    "              f\"{result['test_cer']:<10.4f}\")\n",
    "    \n",
    "    # Announce best model\n",
    "    if experiment_runner.best_model:\n",
    "        best_config = experiment_runner.best_model['config']\n",
    "        best_val_acc = experiment_runner.best_model['val_results']['accuracy']\n",
    "        print(f\"\\nüèÜ BEST MODEL: {best_config['name']}\")\n",
    "        print(f\"   Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "        print(f\"   Model saved as: {experiment_runner.best_model['model_path']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ EXPERIMENTATION COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0d21d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 13: Simple Inference Function for Testing\n",
    "\n",
    "# =============================================================================\n",
    "# SIMPLE INFERENCE FUNCTION - USE TRAINED MODELS FOR TRANSLATION\n",
    "# =============================================================================\n",
    "\n",
    "def translate_urdu_poetry(model, src_tokenizer, tgt_tokenizer, urdu_text, device='cuda', max_length=100):\n",
    "    \"\"\"\n",
    "    Simple function to translate Urdu poetry using trained model\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Seq2Seq model\n",
    "        src_tokenizer: Source (Urdu) tokenizer\n",
    "        tgt_tokenizer: Target (Roman Urdu) tokenizer\n",
    "        urdu_text: Input Urdu text to translate\n",
    "        device: Device to run inference on\n",
    "        max_length: Maximum length of generated translation\n",
    "    \n",
    "    Returns:\n",
    "        dict: Translation results\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Clean the input text\n",
    "    cleaner = TextCleaner()\n",
    "    cleaned_urdu = cleaner.clean_urdu(urdu_text)\n",
    "    \n",
    "    print(f\"üî§ Input Urdu: {cleaned_urdu}\")\n",
    "    \n",
    "    # Debug: Check tokenizer vocabulary\n",
    "    print(f\"üîç Tokenizer vocab size: {src_tokenizer.get_vocab_size()}\")\n",
    "    print(f\"üîç Sample vocab entries: {list(src_tokenizer.vocab.items())[:10]}\")\n",
    "    \n",
    "    # Tokenize input\n",
    "    src_tokens = src_tokenizer.encode(cleaned_urdu)\n",
    "    src_tensor = torch.tensor([src_tokens], dtype=torch.long).to(device)\n",
    "    src_lengths = torch.tensor([len(src_tokens)], dtype=torch.long).to(device)\n",
    "    \n",
    "    print(f\"üìù Tokenized input: {src_tokens}\")\n",
    "    \n",
    "    # Debug: Check if all tokens are <unk>\n",
    "    unk_token_id = src_tokenizer.vocab.get('<unk>', 1)\n",
    "    if all(token == unk_token_id for token in src_tokens):\n",
    "        print(\"‚ö†Ô∏è WARNING: All tokens are <unk>! Tokenizer may not be properly trained on Urdu vocabulary.\")\n",
    "        print(\"üí° This means the model can't understand the input text.\")\n",
    "        print(\"üîß Attempting to use word-level tokenization as fallback...\")\n",
    "        \n",
    "        # Fallback: Try word-level tokenization\n",
    "        words = cleaned_urdu.split()\n",
    "        fallback_tokens = []\n",
    "        for word in words:\n",
    "            if word in src_tokenizer.vocab:\n",
    "                fallback_tokens.append(src_tokenizer.vocab[word])\n",
    "            else:\n",
    "                fallback_tokens.append(unk_token_id)\n",
    "        \n",
    "        if fallback_tokens != src_tokens:\n",
    "            print(f\"üîÑ Using fallback tokenization: {fallback_tokens}\")\n",
    "            src_tokens = fallback_tokens\n",
    "            src_tensor = torch.tensor([src_tokens], dtype=torch.long).to(device)\n",
    "            src_lengths = torch.tensor([len(src_tokens)], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Initialize target sequence with SOS token\n",
    "    sos_token = tgt_tokenizer.vocab.get('<sos>', 1)\n",
    "    eos_token = tgt_tokenizer.vocab.get('<eos>', 2)\n",
    "    \n",
    "    # Start with SOS token\n",
    "    target_sequence = [sos_token]\n",
    "    target_tensor = torch.tensor([target_sequence], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Generate translation using greedy decoding\n",
    "    with torch.no_grad():\n",
    "        for step in range(max_length):\n",
    "            # Forward pass\n",
    "            outputs = model(src_tensor, target_tensor, src_lengths, teacher_forcing_ratio=0.0)\n",
    "            \n",
    "            # Get the last predicted token\n",
    "            next_token_logits = outputs[0, -1, :]\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1).item()\n",
    "            \n",
    "            # Stop if EOS token is generated\n",
    "            if next_token == eos_token:\n",
    "                break\n",
    "            \n",
    "            # Add token to sequence\n",
    "            target_sequence.append(next_token)\n",
    "            target_tensor = torch.tensor([target_sequence], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Decode the generated sequence\n",
    "    generated_text = tgt_tokenizer.decode(target_sequence)\n",
    "    \n",
    "    # Calculate confidence (average probability of generated tokens)\n",
    "    with torch.no_grad():\n",
    "        final_outputs = model(src_tensor, target_tensor, src_lengths, teacher_forcing_ratio=0.0)\n",
    "        probabilities = torch.softmax(final_outputs, dim=-1)\n",
    "        confidence = torch.mean(torch.max(probabilities, dim=-1)[0]).item()\n",
    "    \n",
    "    result = {\n",
    "        'source_urdu': urdu_text,\n",
    "        'cleaned_urdu': cleaned_urdu,\n",
    "        'translation': generated_text,\n",
    "        'confidence': confidence,\n",
    "        'tokens_generated': len(target_sequence),\n",
    "        'model_used': 'Trained NMT Model'\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "def load_trained_model(model_path, device='cuda'):\n",
    "    \"\"\"\n",
    "    Load a trained model from checkpoint\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the model checkpoint\n",
    "        device: Device to load model on\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (model, config)\n",
    "    \"\"\"\n",
    "    print(f\"üìÇ Loading model from: {model_path}\")\n",
    "    \n",
    "    # Load checkpoint (fix for PyTorch 2.6+ weights_only security)\n",
    "    checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "    config = checkpoint.get('config', {})\n",
    "    \n",
    "    print(f\"üîß Model config: {config}\")\n",
    "    \n",
    "    # Extract model parameters from the saved state_dict\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    \n",
    "    # Extract vocabulary sizes from embedding layers\n",
    "    src_vocab_size = state_dict['encoder.embedding.weight'].shape[0]\n",
    "    tgt_vocab_size = state_dict['decoder.embedding.weight'].shape[0]\n",
    "    \n",
    "    # Extract embedding dimension\n",
    "    embed_dim = state_dict['encoder.embedding.weight'].shape[1]\n",
    "    \n",
    "    # Extract hidden dimension from LSTM weights\n",
    "    hidden_dim = state_dict['encoder.lstm.weight_ih_l0'].shape[0] // 4  # LSTM has 4 gates\n",
    "    \n",
    "    # Count encoder layers by counting weight_ih_l* keys\n",
    "    encoder_layers = max([int(k.split('_l')[1].split('_')[0]) for k in state_dict.keys() \n",
    "                         if 'encoder.lstm.weight_ih_l' in k and '_reverse' not in k]) + 1\n",
    "    \n",
    "    # Count decoder layers by counting weight_ih_l* keys\n",
    "    decoder_layers = max([int(k.split('_l')[1].split('_')[0]) for k in state_dict.keys() \n",
    "                         if 'decoder.lstm.weight_ih_l' in k]) + 1\n",
    "    \n",
    "    print(f\"üîç Extracted model parameters:\")\n",
    "    print(f\"  ‚Ä¢ src_vocab_size: {src_vocab_size}\")\n",
    "    print(f\"  ‚Ä¢ tgt_vocab_size: {tgt_vocab_size}\")\n",
    "    print(f\"  ‚Ä¢ embed_dim: {embed_dim}\")\n",
    "    print(f\"  ‚Ä¢ hidden_dim: {hidden_dim}\")\n",
    "    print(f\"  ‚Ä¢ encoder_layers: {encoder_layers}\")\n",
    "    print(f\"  ‚Ä¢ decoder_layers: {decoder_layers}\")\n",
    "    \n",
    "    # Create model with extracted parameters\n",
    "    model = Seq2SeqModel(\n",
    "        src_vocab_size=src_vocab_size,\n",
    "        tgt_vocab_size=tgt_vocab_size,\n",
    "        embed_dim=embed_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        encoder_layers=encoder_layers,\n",
    "        decoder_layers=decoder_layers,\n",
    "        dropout=config.get('dropout', 0.3)\n",
    "    )\n",
    "    \n",
    "    # Load model weights\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"‚úÖ Model loaded successfully!\")\n",
    "    print(f\"üìä Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    return model, config\n",
    "\n",
    "def test_translation_examples():\n",
    "    \"\"\"\n",
    "    Test the translation function with example Urdu poetry\n",
    "    \"\"\"\n",
    "    print(\"\\nüéØ TESTING TRANSLATION FUNCTION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Example Urdu poetry\n",
    "    urdu_poems = [\n",
    "        \"€å€Å ŸÜ€Å ÿ™⁄æ€å €ÅŸÖÿßÿ±€å ŸÇÿ≥ŸÖÿ™ ⁄©€Å ŸàÿµÿßŸÑ €åÿßÿ± €ÅŸàÿ™ÿß\",\n",
    "        \"ÿß⁄Øÿ± ÿßŸæŸÜÿß ⁄©€Åÿß ÿ¢Ÿæ €Å€å ÿ≥ŸÖÿ¨⁄æÿ™€í ÿ™Ÿà ⁄©€åÿß ⁄©€Åÿ™€í\",\n",
    "        \"ÿØŸÑ ÿ≥€í ÿ¨Ÿà ÿ®ÿßÿ™ ŸÜ⁄©ŸÑÿ™€å €Å€í ÿßÿ´ÿ± ÿ±⁄©⁄æÿ™€å €Å€í\"\n",
    "    ]\n",
    "    \n",
    "    print(\"üìù Example Urdu Poems:\")\n",
    "    for i, poem in enumerate(urdu_poems, 1):\n",
    "        print(f\"  {i}. {poem}\")\n",
    "    \n",
    "    print(\"\\nüí° To use your trained model:\")\n",
    "    print(\"1. Load the best model: model, config = load_trained_model('best_model_exp_2.pth')\")\n",
    "    print(\"2. Load tokenizers from the experiment directory\")\n",
    "    print(\"3. Translate poetry: result = translate_urdu_poetry(model, src_tokenizer, tgt_tokenizer, '€å€Å ŸÜ€Å ÿ™⁄æ€å €ÅŸÖÿßÿ±€å ŸÇÿ≥ŸÖÿ™')\")\n",
    "    print(\"4. Get translation: print(result['translation'])\")\n",
    "    \n",
    "    print(\"\\nüöÄ Example usage code:\")\n",
    "    print(\"\"\"\n",
    "# Load your best trained model\n",
    "model, config = load_trained_model('best_model_exp_2.pth')\n",
    "\n",
    "# Load tokenizers (you'll need to load them from the experiment directory)\n",
    "# src_tokenizer, tgt_tokenizer = load_tokenizers_from_experiment('exp_2')\n",
    "\n",
    "# Translate a poem\n",
    "result = translate_urdu_poetry(\n",
    "    model, src_tokenizer, tgt_tokenizer, \n",
    "    \"€å€Å ŸÜ€Å ÿ™⁄æ€å €ÅŸÖÿßÿ±€å ŸÇÿ≥ŸÖÿ™ ⁄©€Å ŸàÿµÿßŸÑ €åÿßÿ± €ÅŸàÿ™ÿß\"\n",
    ")\n",
    "\n",
    "print(f\"Translation: {result['translation']}\")\n",
    "print(f\"Confidence: {result['confidence']:.3f}\")\n",
    "\"\"\")\n",
    "\n",
    "# Run the test\n",
    "test_translation_examples()\n",
    "\n",
    "print(\"\\n=== COMPLETE ERROR-FREE SOLUTION READY ===\")\n",
    "print(\"Copy each cell block (between triple quotes) into separate Colab cells\")\n",
    "print(\"Run them in order for guaranteed success!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae6cad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 14: Interactive User Input Function for Testing\n",
    "\n",
    "def interactive_translation_test():\n",
    "    \"\"\"\n",
    "    Interactive function to test the trained model with user input\n",
    "    \"\"\"\n",
    "    print(\"üéØ INTERACTIVE URDU TO ROMAN URDU TRANSLATION TEST\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Check if any trained models exist\n",
    "    model_files = []\n",
    "    for i in range(1, 4):  # Check for exp 1, 2, 3\n",
    "        model_path = f'best_model_exp_{i}.pth'\n",
    "        if os.path.exists(model_path):\n",
    "            model_files.append((i, model_path))\n",
    "    \n",
    "    if not model_files:\n",
    "        print(\"‚ùå No trained models found!\")\n",
    "        print(\"üí° Please run the experiments first (CELL 12) to train models\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üìÇ Found {len(model_files)} trained model(s):\")\n",
    "    for exp_num, model_path in model_files:\n",
    "        print(f\"  - Experiment {exp_num}: {model_path}\")\n",
    "    \n",
    "    # Let user choose model\n",
    "    if len(model_files) == 1:\n",
    "        chosen_exp, chosen_model = model_files[0]\n",
    "        print(f\"\\n‚úÖ Using: {chosen_model}\")\n",
    "    else:\n",
    "        print(f\"\\nü§î Which model would you like to use?\")\n",
    "        for i, (exp_num, model_path) in enumerate(model_files, 1):\n",
    "            print(f\"  {i}. Experiment {exp_num}\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                choice = int(input(\"Enter your choice (1-{}): \".format(len(model_files))))\n",
    "                if 1 <= choice <= len(model_files):\n",
    "                    chosen_exp, chosen_model = model_files[choice - 1]\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"‚ùå Invalid choice! Please try again.\")\n",
    "            except ValueError:\n",
    "                print(\"‚ùå Please enter a valid number!\")\n",
    "    \n",
    "    print(f\"\\nüîÑ Loading model: {chosen_model}\")\n",
    "    \n",
    "    try:\n",
    "        # Load the chosen model\n",
    "        model, config = load_trained_model(chosen_model, device)\n",
    "        \n",
    "        # Load tokenizers for this experiment\n",
    "        tokenizer_dir = f\"/content/tokenizer_checkpoints_exp_{chosen_exp}\"\n",
    "        tokenizer_files = []\n",
    "        \n",
    "        if os.path.exists(tokenizer_dir):\n",
    "            for root, dirs, files in os.walk(tokenizer_dir):\n",
    "                for file in files:\n",
    "                    if file.endswith('.pkl'):\n",
    "                        tokenizer_files.append(os.path.join(root, file))\n",
    "        \n",
    "        if not tokenizer_files:\n",
    "            print(\"‚ùå No tokenizer files found!\")\n",
    "            print(\"üí° Please ensure tokenizers were saved during training\")\n",
    "            return\n",
    "        \n",
    "        # Load tokenizers\n",
    "        with open(tokenizer_files[0], 'rb') as f:\n",
    "            tokenizer_data = pickle.load(f)\n",
    "            src_tokenizer = tokenizer_data['src_tokenizer']\n",
    "            tgt_tokenizer = tokenizer_data['tgt_tokenizer']\n",
    "        \n",
    "        print(\"‚úÖ Model and tokenizers loaded successfully!\")\n",
    "        print(f\"üìä Model config: embed_dim={config.get('embed_dim', 'N/A')}, hidden_dim={config.get('hidden_dim', 'N/A')}\")\n",
    "        \n",
    "        # Debug: Check tokenizer info\n",
    "        print(f\"üîç Source tokenizer vocab size: {src_tokenizer.get_vocab_size()}\")\n",
    "        print(f\"üîç Target tokenizer vocab size: {tgt_tokenizer.get_vocab_size()}\")\n",
    "        print(f\"üîç Source tokenizer sample words: {list(src_tokenizer.vocab.keys())[:10]}\")\n",
    "        print(f\"üîç Target tokenizer sample words: {list(tgt_tokenizer.vocab.keys())[:10]}\")\n",
    "        \n",
    "        # Interactive translation loop\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üöÄ READY FOR TRANSLATION!\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"üí° Enter Urdu text to translate (type 'quit' to exit)\")\n",
    "        print(\"üí° Example: €å€Å ŸÜ€Å ÿ™⁄æ€å €ÅŸÖÿßÿ±€å ŸÇÿ≥ŸÖÿ™ ⁄©€Å ŸàÿµÿßŸÑ €åÿßÿ± €ÅŸàÿ™ÿß\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                # Get user input\n",
    "                urdu_text = input(\"\\nüìù Enter Urdu text: \").strip()\n",
    "                \n",
    "                # Check for exit command\n",
    "                if urdu_text.lower() in ['quit', 'exit', 'q', '']:\n",
    "                    print(\"üëã Goodbye!\")\n",
    "                    break\n",
    "                \n",
    "                if not urdu_text:\n",
    "                    print(\"‚ö†Ô∏è Please enter some text!\")\n",
    "                    continue\n",
    "                \n",
    "                print(f\"\\nüîÑ Translating: {urdu_text}\")\n",
    "                print(\"-\" * 40)\n",
    "                \n",
    "                # Translate the text\n",
    "                result = translate_urdu_poetry(\n",
    "                    model, src_tokenizer, tgt_tokenizer, \n",
    "                    urdu_text, device=device\n",
    "                )\n",
    "                \n",
    "                # Display results\n",
    "                print(f\"üì§ Translation: {result['translation']}\")\n",
    "                print(f\"üéØ Confidence: {result['confidence']:.3f}\")\n",
    "                print(f\"üìä Tokens Generated: {result['tokens_generated']}\")\n",
    "                \n",
    "                # Ask if user wants to continue\n",
    "                continue_translation = input(\"\\nüîÑ Translate another text? (y/n): \").strip().lower()\n",
    "                if continue_translation not in ['y', 'yes', '']:\n",
    "                    print(\"üëã Goodbye!\")\n",
    "                    break\n",
    "                    \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\nüëã Goodbye!\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error during translation: {e}\")\n",
    "                print(\"üí° Please try again with different text\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading model: {e}\")\n",
    "        print(\"üí° Please ensure the model file is valid and complete\")\n",
    "\n",
    "def quick_test_examples():\n",
    "    \"\"\"\n",
    "    Quick test with predefined examples\n",
    "    \"\"\"\n",
    "    print(\"üéØ QUICK TEST WITH PREDEFINED EXAMPLES\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Example Urdu poetry\n",
    "    examples = [\n",
    "        \"€å€Å ŸÜ€Å ÿ™⁄æ€å €ÅŸÖÿßÿ±€å ŸÇÿ≥ŸÖÿ™ ⁄©€Å ŸàÿµÿßŸÑ €åÿßÿ± €ÅŸàÿ™ÿß\",\n",
    "        \"ÿß⁄Øÿ± ÿßŸæŸÜÿß ⁄©€Åÿß ÿ¢Ÿæ €Å€å ÿ≥ŸÖÿ¨⁄æÿ™€í ÿ™Ÿà ⁄©€åÿß ⁄©€Åÿ™€í\", \n",
    "        \"ÿØŸÑ ÿ≥€í ÿ¨Ÿà ÿ®ÿßÿ™ ŸÜ⁄©ŸÑÿ™€å €Å€í ÿßÿ´ÿ± ÿ±⁄©⁄æÿ™€å €Å€í\",\n",
    "        \"€ÅŸÖ ⁄©Ÿà ŸÖÿπŸÑŸàŸÖ €Å€í ÿ¨ŸÜÿ™ ⁄©€å ÿ≠ŸÇ€åŸÇÿ™ ŸÑ€å⁄©ŸÜ\",\n",
    "        \"ÿπÿ¥ŸÇ ŸÖ€å⁄∫ ÿ∫ŸÖ ⁄©ÿß ŸÖÿ≤€Å ÿ®⁄æ€å ŸÜ€Å€å⁄∫ ÿ¢ÿ™ÿß\"\n",
    "    ]\n",
    "    \n",
    "    print(\"üìù Testing with example Urdu poetry:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Check for available models\n",
    "    model_files = []\n",
    "    for i in range(1, 4):\n",
    "        model_path = f'best_model_exp_{i}.pth'\n",
    "        if os.path.exists(model_path):\n",
    "            model_files.append((i, model_path))\n",
    "    \n",
    "    if not model_files:\n",
    "        print(\"‚ùå No trained models found!\")\n",
    "        print(\"üí° Please run the experiments first (CELL 12) to train models\")\n",
    "        return\n",
    "    \n",
    "    # Use the first available model\n",
    "    chosen_exp, chosen_model = model_files[0]\n",
    "    print(f\"üîÑ Using model: {chosen_model}\")\n",
    "    \n",
    "    try:\n",
    "        # Load model and tokenizers\n",
    "        model, config = load_trained_model(chosen_model, device)\n",
    "        \n",
    "        # Load tokenizers\n",
    "        tokenizer_dir = f\"/content/tokenizer_checkpoints_exp_{chosen_exp}\"\n",
    "        tokenizer_files = []\n",
    "        \n",
    "        if os.path.exists(tokenizer_dir):\n",
    "            for root, dirs, files in os.walk(tokenizer_dir):\n",
    "                for file in files:\n",
    "                    if file.endswith('.pkl'):\n",
    "                        tokenizer_files.append(os.path.join(root, file))\n",
    "        \n",
    "        if tokenizer_files:\n",
    "            with open(tokenizer_files[0], 'rb') as f:\n",
    "                tokenizer_data = pickle.load(f)\n",
    "                src_tokenizer = tokenizer_data['src_tokenizer']\n",
    "                tgt_tokenizer = tokenizer_data['tgt_tokenizer']\n",
    "            \n",
    "            print(\"‚úÖ Model loaded successfully!\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            # Test each example\n",
    "            for i, example in enumerate(examples, 1):\n",
    "                print(f\"\\nüìù Example {i}: {example}\")\n",
    "                \n",
    "                result = translate_urdu_poetry(\n",
    "                    model, src_tokenizer, tgt_tokenizer, \n",
    "                    example, device=device\n",
    "                )\n",
    "                \n",
    "                print(f\"üì§ Translation: {result['translation']}\")\n",
    "                print(f\"üéØ Confidence: {result['confidence']:.3f}\")\n",
    "                print(\"-\" * 30)\n",
    "        \n",
    "        else:\n",
    "            print(\"‚ùå Tokenizer files not found!\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "# Main menu function\n",
    "def main_test_menu():\n",
    "    \"\"\"\n",
    "    Main menu for testing the trained model\n",
    "    \"\"\"\n",
    "    print(\"\\nüéØ URDU TO ROMAN URDU TRANSLATION TESTING\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"Choose testing option:\")\n",
    "    print(\"1. Interactive Translation (Enter your own text)\")\n",
    "    print(\"2. Quick Test (Predefined examples)\")\n",
    "    print(\"3. Exit\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            choice = input(\"Enter your choice (1-3): \").strip()\n",
    "            \n",
    "            if choice == '1':\n",
    "                interactive_translation_test()\n",
    "                break\n",
    "            elif choice == '2':\n",
    "                quick_test_examples()\n",
    "                break\n",
    "            elif choice == '3':\n",
    "                print(\"üëã Goodbye!\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"‚ùå Invalid choice! Please enter 1, 2, or 3.\")\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nüëã Goodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "# Run the main menu\n",
    "print(\"üöÄ READY TO TEST YOUR TRAINED MODEL!\")\n",
    "print(\"üí° Make sure you have trained models by running CELL 12 first\")\n",
    "print(\"üí° Then run this cell to test your model interactively\")\n",
    "\n",
    "# Uncomment the line below to run the test menu automatically\n",
    "# main_test_menu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679edbbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
