{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# Neural Machine Translation: Urdu to Roman Urdu\n",
        "## BiLSTM Encoder-Decoder Architecture\n",
        "\n",
        "**Assignment**: Project1 - Neural Machine Translation (15 Abs)  \n",
        "**Objective**: Build a sequence-to-sequence model using BiLSTM encoder-decoder to translate Urdu text into Roman Urdu transliteration.\n",
        "\n",
        "**Architecture**:\n",
        "- Encoder: 2-layer Bidirectional LSTM\n",
        "- Decoder: 4-layer LSTM\n",
        "- Custom BPE Tokenization (implemented from scratch)\n",
        "\n",
        "**Dataset**: urdu_ghazals_rekhta - Classical Urdu poetry with Roman transliterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install torch torchtext nltk sacrebleu editdistance streamlit\n",
        "%pip install matplotlib seaborn tqdm pandas numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "from collections import Counter, defaultdict\n",
        "import random\n",
        "from typing import List, Tuple, Dict, Any\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dataset"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# DATASET EXTRACTION AND SETUP\n",
        "# =============================================================================\n",
        "\n",
        "import zipfile\n",
        "\n",
        "# Clone the dataset repository\n",
        "!git clone https://github.com/amir9ume/urdu_ghazals_rekhta.git\n",
        "\n",
        "print(\"ðŸ”§ EXTRACTING DATASET FROM ZIP FILE...\")\n",
        "\n",
        "# Extract the dataset zip file\n",
        "zip_path = '/content/urdu_ghazals_rekhta/dataset/dataset.zip'\n",
        "extract_to = '/content/dataset_extracted'\n",
        "\n",
        "if not os.path.exists(extract_to):\n",
        "    print(f\"ðŸ“¦ Extracting {zip_path}\")\n",
        "    os.makedirs(extract_to, exist_ok=True)\n",
        "    \n",
        "    if os.path.exists(zip_path):\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_to)\n",
        "        print(\"âœ… Extraction completed!\")\n",
        "    else:\n",
        "        print(f\"âŒ Zip file not found at {zip_path}\")\n",
        "        # Try alternative paths\n",
        "        alt_paths = [\n",
        "            '/content/urdu_ghazals_rekhta/dataset.zip',\n",
        "            '/content/dataset/dataset.zip',\n",
        "            '/content/dataset.zip'\n",
        "        ]\n",
        "        \n",
        "        for alt_path in alt_paths:\n",
        "            if os.path.exists(alt_path):\n",
        "                zip_path = alt_path\n",
        "                print(f\"âœ… Found zip at: {zip_path}\")\n",
        "                with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                    zip_ref.extractall(extract_to)\n",
        "                print(\"âœ… Extraction completed!\")\n",
        "                break\n",
        "else:\n",
        "    print(\"âœ… Dataset already extracted!\")\n",
        "\n",
        "# Find the dataset directory with poets\n",
        "dataset_path = None\n",
        "for root, dirs, files in os.walk(extract_to):\n",
        "    if any(poet in dirs for poet in ['mirza-ghalib', 'ahmad-faraz', 'allama-iqbal']):\n",
        "        dataset_path = root\n",
        "        break\n",
        "\n",
        "if dataset_path:\n",
        "    print(f\"ðŸŽ¯ Dataset found at: {dataset_path}\")\n",
        "    poets = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
        "    print(f\"ðŸ“š Found {len(poets)} poets\")\n",
        "    \n",
        "    # Test a sample poet\n",
        "    if poets:\n",
        "        sample_poet = poets[0]\n",
        "        poet_path = os.path.join(dataset_path, sample_poet)\n",
        "        subdirs = os.listdir(poet_path)\n",
        "        print(f\"ðŸ“– Sample poet '{sample_poet}' structure: {subdirs}\")\n",
        "else:\n",
        "    print(\"âŒ Could not find dataset with poets!\")\n",
        "    # Fallback to original paths\n",
        "    if os.path.exists('/content/urdu_ghazals_rekhta/dataset'):\n",
        "        dataset_path = '/content/urdu_ghazals_rekhta/dataset'\n",
        "    elif os.path.exists('urdu_ghazals_rekhta/dataset'):\n",
        "        dataset_path = 'urdu_ghazals_rekhta/dataset'\n",
        "    else:\n",
        "        dataset_path = 'dataset/dataset'\n",
        "    print(f\"Using fallback path: {dataset_path}\")\n",
        "\n",
        "print(f\"âœ… Final dataset path: {dataset_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header_data_loader"
      },
      "source": [
        "## Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "data_loader"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import unicodedata\n",
        "from tqdm import tqdm\n",
        "from typing import List, Tuple\n",
        "import random\n",
        "\n",
        "class TextCleaner:\n",
        "    \"\"\"\n",
        "    Text cleaning and normalization utilities\n",
        "    \"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def clean_urdu(text: str) -> str:\n",
        "        \"\"\"\n",
        "        Clean and normalize Urdu text\n",
        "        \"\"\"\n",
        "        # Normalize Unicode\n",
        "        text = unicodedata.normalize('NFKC', text)\n",
        "        \n",
        "        # Remove extra whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        \n",
        "        # Remove unwanted punctuation but keep essential ones\n",
        "        text = re.sub(r'[^\\u0600-\\u06FF\\u0750-\\u077F\\u08A0-\\u08FF\\sÛ”ØŒØØŽØØŸ!]', '', text)\n",
        "        \n",
        "        return text.strip()\n",
        "    \n",
        "    @staticmethod\n",
        "    def clean_roman(text: str) -> str:\n",
        "        \"\"\"\n",
        "        Clean and normalize Roman Urdu text\n",
        "        \"\"\"\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "        \n",
        "        # Remove extra whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        \n",
        "        # Keep only alphanumeric, spaces, and basic punctuation\n",
        "        text = re.sub(r'[^a-zA-ZÄÄ«Å«Ä€ÄªÅªÃ±á¹‡á¹›á¹­á¸á¹£Ä¡á¸¥á¸³áº“áº•\\s\\'\\-\\.]', '', text)\n",
        "        \n",
        "        return text.strip()\n",
        "    \n",
        "    @staticmethod\n",
        "    def add_special_tokens(text: str, is_target: bool = False) -> str:\n",
        "        \"\"\"\n",
        "        Add special tokens for sequence processing\n",
        "        \"\"\"\n",
        "        if is_target:\n",
        "            return f\"<sos> {text} <eos>\"\n",
        "        return text\n",
        "\n",
        "def load_dataset(data_path='dataset/dataset'):\n",
        "    \"\"\"\n",
        "    Load Urdu-Roman Urdu parallel corpus from the dataset\n",
        "    \"\"\"\n",
        "    urdu_texts = []\n",
        "    roman_texts = []\n",
        "    \n",
        "    # Get all poet directories\n",
        "    poets = [d for d in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, d))]\n",
        "    print(f\"Found {len(poets)} poets in dataset\")\n",
        "    \n",
        "    for poet in tqdm(poets, desc=\"Loading poets\"):\n",
        "        poet_path = os.path.join(data_path, poet)\n",
        "        urdu_path = os.path.join(poet_path, 'ur')\n",
        "        english_path = os.path.join(poet_path, 'en')\n",
        "        \n",
        "        if os.path.exists(urdu_path) and os.path.exists(english_path):\n",
        "            # Get all files in both directories\n",
        "            urdu_files = set(os.listdir(urdu_path))\n",
        "            english_files = set(os.listdir(english_path))\n",
        "            \n",
        "            # Find common files\n",
        "            common_files = urdu_files.intersection(english_files)\n",
        "            \n",
        "            for file_name in common_files:\n",
        "                try:\n",
        "                    # Read Urdu text\n",
        "                    with open(os.path.join(urdu_path, file_name), 'r', encoding='utf-8') as f:\n",
        "                        urdu_content = f.read().strip()\n",
        "                    \n",
        "                    # Read Roman Urdu text\n",
        "                    with open(os.path.join(english_path, file_name), 'r', encoding='utf-8') as f:\n",
        "                        roman_content = f.read().strip()\n",
        "                    \n",
        "                    # Split into lines and pair them\n",
        "                    urdu_lines = [line.strip() for line in urdu_content.split('\\n') if line.strip()]\n",
        "                    roman_lines = [line.strip() for line in roman_content.split('\\n') if line.strip()]\n",
        "                    \n",
        "                    # Ensure same number of lines\n",
        "                    min_lines = min(len(urdu_lines), len(roman_lines))\n",
        "                    for i in range(min_lines):\n",
        "                        if urdu_lines[i] and roman_lines[i]:\n",
        "                            urdu_texts.append(urdu_lines[i])\n",
        "                            roman_texts.append(roman_lines[i])\n",
        "                            \n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {poet}/{file_name}: {e}\")\n",
        "                    continue\n",
        "    \n",
        "    print(f\"\\nDataset loaded:\")\n",
        "    print(f\"Total pairs: {len(urdu_texts)}\")\n",
        "    \n",
        "    return urdu_texts, roman_texts\n",
        "\n",
        "def clean_and_split_data(urdu_texts: List[str], roman_texts: List[str], \n",
        "                        train_ratio: float = 0.5, val_ratio: float = 0.25, test_ratio: float = 0.25):\n",
        "    \"\"\"\n",
        "    Clean data and split into train/val/test sets\n",
        "    \"\"\"\n",
        "    cleaner = TextCleaner()\n",
        "    \n",
        "    print(\"Cleaning texts...\")\n",
        "    cleaned_urdu = [cleaner.clean_urdu(text) for text in tqdm(urdu_texts, desc=\"Cleaning Urdu\")]\n",
        "    cleaned_roman = [cleaner.clean_roman(text) for text in tqdm(roman_texts, desc=\"Cleaning Roman\")]\n",
        "    \n",
        "    # Filter out empty pairs and very short/long sequences\n",
        "    valid_pairs = []\n",
        "    for u, r in zip(cleaned_urdu, cleaned_roman):\n",
        "        if u and r and 2 <= len(u.split()) <= 50 and 2 <= len(r.split()) <= 50:\n",
        "            valid_pairs.append((u, r))\n",
        "    \n",
        "    print(f\"After cleaning and filtering: {len(valid_pairs)} valid pairs\")\n",
        "    \n",
        "    # Shuffle data\n",
        "    random.shuffle(valid_pairs)\n",
        "    \n",
        "    # Split data\n",
        "    total = len(valid_pairs)\n",
        "    train_end = int(total * train_ratio)\n",
        "    val_end = train_end + int(total * val_ratio)\n",
        "    \n",
        "    train_pairs = valid_pairs[:train_end]\n",
        "    val_pairs = valid_pairs[train_end:val_end]\n",
        "    test_pairs = valid_pairs[val_end:]\n",
        "    \n",
        "    print(f\"Data splits:\")\n",
        "    print(f\"  Train: {len(train_pairs)} pairs\")\n",
        "    print(f\"  Validation: {len(val_pairs)} pairs\")\n",
        "    print(f\"  Test: {len(test_pairs)} pairs\")\n",
        "    \n",
        "    return train_pairs, val_pairs, test_pairs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header_bpe_tokenizer"
      },
      "source": [
        "## Bpe Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpe_tokenizer"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "from typing import List, Dict, Tuple, Set\n",
        "import pickle\n",
        "\n",
        "class BPETokenizer:\n",
        "    \"\"\"\n",
        "    Byte-Pair Encoding tokenizer implemented from scratch\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size: int = 10000):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.word_freqs = Counter()\n",
        "        self.vocab = {}\n",
        "        self.merges = []\n",
        "        self.special_tokens = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
        "        \n",
        "    def _get_stats(self, vocab: Dict[str, int]) -> Dict[Tuple[str, str], int]:\n",
        "        \"\"\"\n",
        "        Get frequency of consecutive symbol pairs\n",
        "        \"\"\"\n",
        "        pairs = defaultdict(int)\n",
        "        for word, freq in vocab.items():\n",
        "            symbols = word.split()\n",
        "            for i in range(len(symbols) - 1):\n",
        "                pairs[symbols[i], symbols[i + 1]] += freq\n",
        "        return pairs\n",
        "    \n",
        "    def _merge_vocab(self, pair: Tuple[str, str], vocab: Dict[str, int]) -> Dict[str, int]:\n",
        "        \"\"\"\n",
        "        Merge the most frequent pair in the vocabulary\n",
        "        \"\"\"\n",
        "        new_vocab = {}\n",
        "        bigram = re.escape(' '.join(pair))\n",
        "        p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "        \n",
        "        for word in vocab:\n",
        "            new_word = p.sub(''.join(pair), word)\n",
        "            new_vocab[new_word] = vocab[word]\n",
        "        return new_vocab\n",
        "    \n",
        "    def train(self, texts: List[str]):\n",
        "        \"\"\"\n",
        "        Train BPE on the given texts\n",
        "        \"\"\"\n",
        "        print(\"Training BPE tokenizer...\")\n",
        "        \n",
        "        # Initialize word frequencies\n",
        "        for text in texts:\n",
        "            words = text.split()\n",
        "            for word in words:\n",
        "                self.word_freqs[word] += 1\n",
        "        \n",
        "        # Initialize vocabulary with character-level splits\n",
        "        vocab = {}\n",
        "        for word, freq in self.word_freqs.items():\n",
        "            # Split word into characters and add end-of-word token\n",
        "            vocab[' '.join(list(word)) + ' </w>'] = freq\n",
        "        \n",
        "        # Add special tokens to vocabulary\n",
        "        for token in self.special_tokens:\n",
        "            vocab[token] = 1\n",
        "        \n",
        "        # Iteratively merge most frequent pairs\n",
        "        num_merges = self.vocab_size - len(self.special_tokens)\n",
        "        \n",
        "        for i in range(num_merges):\n",
        "            pairs = self._get_stats(vocab)\n",
        "            if not pairs:\n",
        "                break\n",
        "                \n",
        "            best_pair = max(pairs, key=pairs.get)\n",
        "            vocab = self._merge_vocab(best_pair, vocab)\n",
        "            self.merges.append(best_pair)\n",
        "            \n",
        "            if (i + 1) % 1000 == 0:\n",
        "                print(f\"Merged {i + 1}/{num_merges} pairs\")\n",
        "        \n",
        "        # Create final vocabulary\n",
        "        self.vocab = {}\n",
        "        for i, token in enumerate(self.special_tokens):\n",
        "            self.vocab[token] = i\n",
        "        \n",
        "        for word in vocab:\n",
        "            if word not in self.vocab:\n",
        "                self.vocab[word] = len(self.vocab)\n",
        "        \n",
        "        print(f\"BPE training completed. Vocabulary size: {len(self.vocab)}\")\n",
        "    \n",
        "    def _get_word_tokens(self, word: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Tokenize a single word using learned BPE merges\n",
        "        \"\"\"\n",
        "        if word in self.vocab:\n",
        "            return [word]\n",
        "        \n",
        "        word = ' '.join(list(word)) + ' </w>'\n",
        "        pairs = self._get_word_pairs(word)\n",
        "        \n",
        "        if not pairs:\n",
        "            return [word]\n",
        "        \n",
        "        while True:\n",
        "            bigram = min(pairs, key=lambda pair: self.merges.index(pair) if pair in self.merges else float('inf'))\n",
        "            if bigram not in self.merges:\n",
        "                break\n",
        "            first, second = bigram\n",
        "            new_word = []\n",
        "            i = 0\n",
        "            while i < len(word):\n",
        "                try:\n",
        "                    j = word.index(first, i)\n",
        "                    new_word.extend(word[i:j])\n",
        "                    i = j\n",
        "                except:\n",
        "                    new_word.extend(word[i:])\n",
        "                    break\n",
        "                \n",
        "                if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n",
        "                    new_word.append(first + second)\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_word.append(word[i])\n",
        "                    i += 1\n",
        "            word = new_word\n",
        "            if len(word) == 1:\n",
        "                break\n",
        "            else:\n",
        "                pairs = self._get_word_pairs(word)\n",
        "        \n",
        "        return word\n",
        "    \n",
        "    def _get_word_pairs(self, word) -> Set[Tuple[str, str]]:\n",
        "        \"\"\"\n",
        "        Get all pairs from a word\n",
        "        \"\"\"\n",
        "        if isinstance(word, str):\n",
        "            word = word.split()\n",
        "        pairs = set()\n",
        "        prev_char = word[0]\n",
        "        for char in word[1:]:\n",
        "            pairs.add((prev_char, char))\n",
        "            prev_char = char\n",
        "        return pairs\n",
        "    \n",
        "    def encode(self, text: str) -> List[int]:\n",
        "        \"\"\"\n",
        "        Encode text to token IDs\n",
        "        \"\"\"\n",
        "        tokens = []\n",
        "        words = text.split()\n",
        "        \n",
        "        for word in words:\n",
        "            word_tokens = self._get_word_tokens(word)\n",
        "            for token in word_tokens:\n",
        "                if token in self.vocab:\n",
        "                    tokens.append(self.vocab[token])\n",
        "                else:\n",
        "                    tokens.append(self.vocab['<unk>'])\n",
        "        \n",
        "        return tokens\n",
        "    \n",
        "    def decode(self, token_ids: List[int]) -> str:\n",
        "        \"\"\"\n",
        "        Decode token IDs back to text\n",
        "        \"\"\"\n",
        "        # Create reverse vocabulary\n",
        "        id_to_token = {v: k for k, v in self.vocab.items()}\n",
        "        \n",
        "        tokens = []\n",
        "        for token_id in token_ids:\n",
        "            if token_id in id_to_token:\n",
        "                token = id_to_token[token_id]\n",
        "                if token not in self.special_tokens:\n",
        "                    tokens.append(token)\n",
        "        \n",
        "        # Join tokens and clean up\n",
        "        text = ' '.join(tokens)\n",
        "        text = text.replace('</w>', ' ')\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return text.strip()\n",
        "    \n",
        "    def save(self, filepath: str):\n",
        "        \"\"\"\n",
        "        Save tokenizer to file\n",
        "        \"\"\"\n",
        "        with open(filepath, 'wb') as f:\n",
        "            pickle.dump({\n",
        "                'vocab': self.vocab,\n",
        "                'merges': self.merges,\n",
        "                'vocab_size': self.vocab_size,\n",
        "                'special_tokens': self.special_tokens\n",
        "            }, f)\n",
        "    \n",
        "    def load(self, filepath: str):\n",
        "        \"\"\"\n",
        "        Load tokenizer from file\n",
        "        \"\"\"\n",
        "        with open(filepath, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "            self.vocab = data['vocab']\n",
        "            self.merges = data['merges']\n",
        "            self.vocab_size = data['vocab_size']\n",
        "            self.special_tokens = data['special_tokens']\n",
        "    \n",
        "    def get_vocab_size(self) -> int:\n",
        "        return len(self.vocab)\n",
        "\n",
        "def create_tokenizers(train_pairs: List[Tuple[str, str]], \n",
        "                     src_vocab_size: int = 8000, \n",
        "                     tgt_vocab_size: int = 8000) -> Tuple[BPETokenizer, BPETokenizer]:\n",
        "    \"\"\"\n",
        "    Create and train source and target tokenizers\n",
        "    \"\"\"\n",
        "    src_texts = [pair[0] for pair in train_pairs]\n",
        "    tgt_texts = [pair[1] for pair in train_pairs]\n",
        "    \n",
        "    # Create tokenizers\n",
        "    src_tokenizer = BPETokenizer(vocab_size=src_vocab_size)\n",
        "    tgt_tokenizer = BPETokenizer(vocab_size=tgt_vocab_size)\n",
        "    \n",
        "    # Train tokenizers\n",
        "    print(\"Training source (Urdu) tokenizer...\")\n",
        "    src_tokenizer.train(src_texts)\n",
        "    \n",
        "    print(\"Training target (Roman Urdu) tokenizer...\")\n",
        "    tgt_tokenizer.train(tgt_texts)\n",
        "    \n",
        "    return src_tokenizer, tgt_tokenizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header_model"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "model"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import random\n",
        "\n",
        "class BiLSTMEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    2-layer Bidirectional LSTM Encoder\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size: int, embed_dim: int, hidden_dim: int, \n",
        "                 num_layers: int = 2, dropout: float = 0.3):\n",
        "        super(BiLSTMEncoder, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, \n",
        "                           batch_first=True, bidirectional=True, dropout=dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x, lengths=None):\n",
        "        # x: (batch_size, seq_len)\n",
        "        embedded = self.dropout(self.embedding(x))\n",
        "        \n",
        "        if lengths is not None:\n",
        "            # Pack padded sequence for efficiency\n",
        "            packed = nn.utils.rnn.pack_padded_sequence(\n",
        "                embedded, lengths, batch_first=True, enforce_sorted=False)\n",
        "            output, (hidden, cell) = self.lstm(packed)\n",
        "            output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
        "        else:\n",
        "            output, (hidden, cell) = self.lstm(embedded)\n",
        "        \n",
        "        # output: (batch_size, seq_len, hidden_dim * 2)\n",
        "        # hidden: (num_layers * 2, batch_size, hidden_dim)\n",
        "        # cell: (num_layers * 2, batch_size, hidden_dim)\n",
        "        \n",
        "        # Combine forward and backward hidden states\n",
        "        # Take the last layer's hidden states\n",
        "        hidden_fwd = hidden[-2]  # Forward direction\n",
        "        hidden_bwd = hidden[-1]  # Backward direction\n",
        "        final_hidden = torch.cat([hidden_fwd, hidden_bwd], dim=1)\n",
        "        \n",
        "        cell_fwd = cell[-2]\n",
        "        cell_bwd = cell[-1]\n",
        "        final_cell = torch.cat([cell_fwd, cell_bwd], dim=1)\n",
        "        \n",
        "        return output, (final_hidden, final_cell)\n",
        "\n",
        "class LSTMDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    4-layer LSTM Decoder with Attention\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size: int, embed_dim: int, hidden_dim: int, \n",
        "                 encoder_hidden_dim: int, num_layers: int = 4, dropout: float = 0.3):\n",
        "        super(LSTMDecoder, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.vocab_size = vocab_size\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        \n",
        "        # Project encoder hidden state to decoder hidden state size\n",
        "        self.hidden_projection = nn.Linear(encoder_hidden_dim, hidden_dim)\n",
        "        self.cell_projection = nn.Linear(encoder_hidden_dim, hidden_dim)\n",
        "        \n",
        "        # Attention mechanism\n",
        "        self.attention = AttentionMechanism(hidden_dim, encoder_hidden_dim * 2)\n",
        "        \n",
        "        # LSTM layers\n",
        "        self.lstm = nn.LSTM(embed_dim + encoder_hidden_dim * 2, hidden_dim, \n",
        "                           num_layers, batch_first=True, dropout=dropout)\n",
        "        \n",
        "        # Output projection\n",
        "        self.output_projection = nn.Linear(hidden_dim, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input_token, hidden, cell, encoder_outputs, mask=None):\n",
        "        # input_token: (batch_size, 1)\n",
        "        # hidden: (num_layers, batch_size, hidden_dim)\n",
        "        # cell: (num_layers, batch_size, hidden_dim)\n",
        "        # encoder_outputs: (batch_size, seq_len, encoder_hidden_dim * 2)\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input_token))\n",
        "        # embedded: (batch_size, 1, embed_dim)\n",
        "        \n",
        "        # Apply attention\n",
        "        context, attention_weights = self.attention(\n",
        "            hidden[-1].unsqueeze(1), encoder_outputs, mask)\n",
        "        # context: (batch_size, 1, encoder_hidden_dim * 2)\n",
        "        \n",
        "        # Concatenate embedding and context\n",
        "        lstm_input = torch.cat([embedded, context], dim=2)\n",
        "        # lstm_input: (batch_size, 1, embed_dim + encoder_hidden_dim * 2)\n",
        "        \n",
        "        output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
        "        # output: (batch_size, 1, hidden_dim)\n",
        "        \n",
        "        # Project to vocabulary size\n",
        "        output = self.output_projection(output)\n",
        "        # output: (batch_size, 1, vocab_size)\n",
        "        \n",
        "        return output, hidden, cell, attention_weights\n",
        "\n",
        "class AttentionMechanism(nn.Module):\n",
        "    \"\"\"\n",
        "    Additive (Bahdanau) Attention Mechanism\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, decoder_hidden_dim: int, encoder_hidden_dim: int):\n",
        "        super(AttentionMechanism, self).__init__()\n",
        "        self.decoder_projection = nn.Linear(decoder_hidden_dim, encoder_hidden_dim)\n",
        "        self.encoder_projection = nn.Linear(encoder_hidden_dim, encoder_hidden_dim)\n",
        "        self.attention_projection = nn.Linear(encoder_hidden_dim, 1)\n",
        "        \n",
        "    def forward(self, decoder_hidden, encoder_outputs, mask=None):\n",
        "        # decoder_hidden: (batch_size, 1, decoder_hidden_dim)\n",
        "        # encoder_outputs: (batch_size, seq_len, encoder_hidden_dim)\n",
        "        \n",
        "        batch_size, seq_len, encoder_dim = encoder_outputs.size()\n",
        "        \n",
        "        # Project decoder hidden state\n",
        "        decoder_proj = self.decoder_projection(decoder_hidden)\n",
        "        # decoder_proj: (batch_size, 1, encoder_hidden_dim)\n",
        "        \n",
        "        # Project encoder outputs\n",
        "        encoder_proj = self.encoder_projection(encoder_outputs)\n",
        "        # encoder_proj: (batch_size, seq_len, encoder_hidden_dim)\n",
        "        \n",
        "        # Calculate attention scores\n",
        "        energy = torch.tanh(decoder_proj + encoder_proj)\n",
        "        # energy: (batch_size, seq_len, encoder_hidden_dim)\n",
        "        \n",
        "        attention_scores = self.attention_projection(energy).squeeze(2)\n",
        "        # attention_scores: (batch_size, seq_len)\n",
        "        \n",
        "        # Apply mask if provided\n",
        "        if mask is not None:\n",
        "            attention_scores = attention_scores.masked_fill(mask == 0, -1e10)\n",
        "        \n",
        "        # Apply softmax to get attention weights\n",
        "        attention_weights = F.softmax(attention_scores, dim=1)\n",
        "        # attention_weights: (batch_size, seq_len)\n",
        "        \n",
        "        # Calculate context vector\n",
        "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)\n",
        "        # context: (batch_size, 1, encoder_hidden_dim)\n",
        "        \n",
        "        return context, attention_weights\n",
        "\n",
        "class Seq2SeqModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Complete Sequence-to-Sequence Model with BiLSTM Encoder and LSTM Decoder\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, src_vocab_size: int, tgt_vocab_size: int, \n",
        "                 embed_dim: int = 256, hidden_dim: int = 512, \n",
        "                 encoder_layers: int = 2, decoder_layers: int = 4,\n",
        "                 dropout: float = 0.3):\n",
        "        super(Seq2SeqModel, self).__init__()\n",
        "        \n",
        "        self.encoder = BiLSTMEncoder(\n",
        "            vocab_size=src_vocab_size,\n",
        "            embed_dim=embed_dim,\n",
        "            hidden_dim=hidden_dim,\n",
        "            num_layers=encoder_layers,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        \n",
        "        self.decoder = LSTMDecoder(\n",
        "            vocab_size=tgt_vocab_size,\n",
        "            embed_dim=embed_dim,\n",
        "            hidden_dim=hidden_dim,\n",
        "            encoder_hidden_dim=hidden_dim * 2,  # Bidirectional\n",
        "            num_layers=decoder_layers,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        \n",
        "        self.tgt_vocab_size = tgt_vocab_size\n",
        "        \n",
        "    def forward(self, src, tgt, src_lengths=None, teacher_forcing_ratio=0.5):\n",
        "        # src: (batch_size, src_seq_len)\n",
        "        # tgt: (batch_size, tgt_seq_len)\n",
        "        \n",
        "        batch_size = src.size(0)\n",
        "        tgt_len = tgt.size(1)\n",
        "        \n",
        "        # Encoder\n",
        "        encoder_outputs, (hidden, cell) = self.encoder(src, src_lengths)\n",
        "        \n",
        "        # Create mask for encoder outputs\n",
        "        if src_lengths is not None:\n",
        "            mask = torch.zeros(batch_size, src.size(1), device=src.device)\n",
        "            for i, length in enumerate(src_lengths):\n",
        "                mask[i, :length] = 1\n",
        "        else:\n",
        "            mask = torch.ones(batch_size, src.size(1), device=src.device)\n",
        "        \n",
        "        # Initialize decoder hidden states\n",
        "        decoder_hidden = self.decoder.hidden_projection(hidden).unsqueeze(0)\n",
        "        decoder_cell = self.decoder.cell_projection(cell).unsqueeze(0)\n",
        "        \n",
        "        # Repeat for all decoder layers\n",
        "        decoder_hidden = decoder_hidden.repeat(self.decoder.num_layers, 1, 1)\n",
        "        decoder_cell = decoder_cell.repeat(self.decoder.num_layers, 1, 1)\n",
        "        \n",
        "        # Prepare outputs tensor\n",
        "        outputs = torch.zeros(batch_size, tgt_len, self.tgt_vocab_size, device=src.device)\n",
        "        \n",
        "        # First input to decoder is SOS token\n",
        "        input_token = tgt[:, 0].unsqueeze(1)  # (batch_size, 1)\n",
        "        \n",
        "        for t in range(1, tgt_len):\n",
        "            output, decoder_hidden, decoder_cell, _ = self.decoder(\n",
        "                input_token, decoder_hidden, decoder_cell, encoder_outputs, mask)\n",
        "            \n",
        "            outputs[:, t] = output.squeeze(1)\n",
        "            \n",
        "            # Teacher forcing\n",
        "            if random.random() < teacher_forcing_ratio:\n",
        "                input_token = tgt[:, t].unsqueeze(1)\n",
        "            else:\n",
        "                input_token = output.argmax(2)\n",
        "        \n",
        "        return outputs\n",
        "    \n",
        "    def inference(self, src, src_tokenizer, tgt_tokenizer, max_length=50):\n",
        "        \"\"\"\n",
        "        Inference method for generating translations\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            batch_size = src.size(0)\n",
        "            device = src.device\n",
        "            \n",
        "            # Encoder\n",
        "            encoder_outputs, (hidden, cell) = self.encoder(src)\n",
        "            \n",
        "            # Create mask\n",
        "            mask = torch.ones(batch_size, src.size(1), device=device)\n",
        "            \n",
        "            # Initialize decoder\n",
        "            decoder_hidden = self.decoder.hidden_projection(hidden).unsqueeze(0)\n",
        "            decoder_cell = self.decoder.cell_projection(cell).unsqueeze(0)\n",
        "            decoder_hidden = decoder_hidden.repeat(self.decoder.num_layers, 1, 1)\n",
        "            decoder_cell = decoder_cell.repeat(self.decoder.num_layers, 1, 1)\n",
        "            \n",
        "            # Start with SOS token\n",
        "            sos_id = tgt_tokenizer.vocab['<sos>']\n",
        "            eos_id = tgt_tokenizer.vocab['<eos>']\n",
        "            \n",
        "            input_token = torch.full((batch_size, 1), sos_id, device=device)\n",
        "            \n",
        "            outputs = []\n",
        "            \n",
        "            for _ in range(max_length):\n",
        "                output, decoder_hidden, decoder_cell, _ = self.decoder(\n",
        "                    input_token, decoder_hidden, decoder_cell, encoder_outputs, mask)\n",
        "                \n",
        "                predicted = output.argmax(2)\n",
        "                outputs.append(predicted)\n",
        "                \n",
        "                input_token = predicted\n",
        "                \n",
        "                # Stop if all sequences have generated EOS\n",
        "                if (predicted == eos_id).all():\n",
        "                    break\n",
        "            \n",
        "            # Concatenate outputs\n",
        "            if outputs:\n",
        "                outputs = torch.cat(outputs, dim=1)\n",
        "            else:\n",
        "                outputs = torch.empty(batch_size, 0, device=device)\n",
        "            \n",
        "            return outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header_dataset"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dataset"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from typing import List, Tuple\n",
        "import random\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset class for Urdu to Roman Urdu translation\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, pairs: List[Tuple[str, str]], src_tokenizer, tgt_tokenizer):\n",
        "        self.pairs = pairs\n",
        "        self.src_tokenizer = src_tokenizer\n",
        "        self.tgt_tokenizer = tgt_tokenizer\n",
        "        \n",
        "        # Tokenize all pairs\n",
        "        self.tokenized_pairs = []\n",
        "        for src_text, tgt_text in pairs:\n",
        "            src_tokens = src_tokenizer.encode(src_text)\n",
        "            tgt_tokens = tgt_tokenizer.encode(f\"<sos> {tgt_text} <eos>\")\n",
        "            \n",
        "            if len(src_tokens) > 0 and len(tgt_tokens) > 0:\n",
        "                self.tokenized_pairs.append((src_tokens, tgt_tokens))\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.tokenized_pairs)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        src_tokens, tgt_tokens = self.tokenized_pairs[idx]\n",
        "        return {\n",
        "            'src': torch.tensor(src_tokens, dtype=torch.long),\n",
        "            'tgt': torch.tensor(tgt_tokens, dtype=torch.long),\n",
        "            'src_len': len(src_tokens),\n",
        "            'tgt_len': len(tgt_tokens)\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Collate function for DataLoader to handle variable length sequences\n",
        "    \"\"\"\n",
        "    src_sequences = [item['src'] for item in batch]\n",
        "    tgt_sequences = [item['tgt'] for item in batch]\n",
        "    src_lengths = [item['src_len'] for item in batch]\n",
        "    tgt_lengths = [item['tgt_len'] for item in batch]\n",
        "    \n",
        "    # Pad sequences\n",
        "    src_padded = pad_sequence(src_sequences, batch_first=True, padding_value=0)\n",
        "    tgt_padded = pad_sequence(tgt_sequences, batch_first=True, padding_value=0)\n",
        "    \n",
        "    return {\n",
        "        'src': src_padded,\n",
        "        'tgt': tgt_padded,\n",
        "        'src_lengths': torch.tensor(src_lengths, dtype=torch.long),\n",
        "        'tgt_lengths': torch.tensor(tgt_lengths, dtype=torch.long)\n",
        "    }\n",
        "\n",
        "def create_data_loaders(train_pairs, val_pairs, test_pairs, src_tokenizer, tgt_tokenizer, \n",
        "                       batch_size=32, num_workers=0):\n",
        "    \"\"\"\n",
        "    Create DataLoaders for train, validation, and test sets\n",
        "    \"\"\"\n",
        "    # Create datasets\n",
        "    train_dataset = TranslationDataset(train_pairs, src_tokenizer, tgt_tokenizer)\n",
        "    val_dataset = TranslationDataset(val_pairs, src_tokenizer, tgt_tokenizer)\n",
        "    test_dataset = TranslationDataset(test_pairs, src_tokenizer, tgt_tokenizer)\n",
        "    \n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=batch_size, shuffle=True, \n",
        "        collate_fn=collate_fn, num_workers=num_workers\n",
        "    )\n",
        "    \n",
        "    val_loader = DataLoader(\n",
        "        val_dataset, batch_size=batch_size, shuffle=False, \n",
        "        collate_fn=collate_fn, num_workers=num_workers\n",
        "    )\n",
        "    \n",
        "    test_loader = DataLoader(\n",
        "        test_dataset, batch_size=batch_size, shuffle=False, \n",
        "        collate_fn=collate_fn, num_workers=num_workers\n",
        "    )\n",
        "    \n",
        "    print(f\"Data loaders created:\")\n",
        "    print(f\"  Train: {len(train_loader)} batches\")\n",
        "    print(f\"  Validation: {len(val_loader)} batches\")\n",
        "    print(f\"  Test: {len(test_loader)} batches\")\n",
        "    \n",
        "    return train_loader, val_loader, test_loader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header_training"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import numpy as np\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "class Trainer:\n",
        "    \"\"\"\n",
        "    Training class for the Seq2Seq model\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model, train_loader, val_loader, src_tokenizer, tgt_tokenizer, \n",
        "                 lr=1e-3, device='cuda'):\n",
        "        self.model = model.to(device)\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.src_tokenizer = src_tokenizer\n",
        "        self.tgt_tokenizer = tgt_tokenizer\n",
        "        self.device = device\n",
        "        \n",
        "        self.criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding\n",
        "        self.optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "        self.scheduler = ReduceLROnPlateau(self.optimizer, mode='min', patience=3, factor=0.5)\n",
        "        \n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.train_perplexities = []\n",
        "        self.val_perplexities = []\n",
        "        \n",
        "    def train_epoch(self, epoch, teacher_forcing_ratio=0.5):\n",
        "        \"\"\"\n",
        "        Train for one epoch\n",
        "        \"\"\"\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        total_tokens = 0\n",
        "        \n",
        "        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch}')\n",
        "        for batch_idx, batch in enumerate(pbar):\n",
        "            src = batch['src'].to(self.device)\n",
        "            tgt = batch['tgt'].to(self.device)\n",
        "            src_lengths = batch['src_lengths'].to(self.device)\n",
        "            \n",
        "            self.optimizer.zero_grad()\n",
        "            \n",
        "            # Forward pass\n",
        "            outputs = self.model(src, tgt, src_lengths, teacher_forcing_ratio)\n",
        "            \n",
        "            # Calculate loss (ignore first token which is SOS)\n",
        "            outputs = outputs[:, 1:].contiguous().view(-1, outputs.size(-1))\n",
        "            targets = tgt[:, 1:].contiguous().view(-1)\n",
        "            \n",
        "            loss = self.criterion(outputs, targets)\n",
        "            \n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "            self.optimizer.step()\n",
        "            \n",
        "            # Statistics\n",
        "            total_loss += loss.item()\n",
        "            total_tokens += targets.ne(0).sum().item()  # Count non-padding tokens\n",
        "            \n",
        "            pbar.set_postfix({'loss': loss.item()})\n",
        "        \n",
        "        avg_loss = total_loss / len(self.train_loader)\n",
        "        perplexity = np.exp(total_loss * len(self.train_loader) / total_tokens)\n",
        "        \n",
        "        return avg_loss, perplexity\n",
        "    \n",
        "    def validate(self):\n",
        "        \"\"\"\n",
        "        Validate the model\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        total_tokens = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(self.val_loader, desc='Validation'):\n",
        "                src = batch['src'].to(self.device)\n",
        "                tgt = batch['tgt'].to(self.device)\n",
        "                src_lengths = batch['src_lengths'].to(self.device)\n",
        "                \n",
        "                outputs = self.model(src, tgt, src_lengths, teacher_forcing_ratio=0)\n",
        "                \n",
        "                outputs = outputs[:, 1:].contiguous().view(-1, outputs.size(-1))\n",
        "                targets = tgt[:, 1:].contiguous().view(-1)\n",
        "                \n",
        "                loss = self.criterion(outputs, targets)\n",
        "                total_loss += loss.item()\n",
        "                total_tokens += targets.ne(0).sum().item()\n",
        "        \n",
        "        avg_loss = total_loss / len(self.val_loader)\n",
        "        perplexity = np.exp(total_loss * len(self.val_loader) / total_tokens)\n",
        "        \n",
        "        return avg_loss, perplexity\n",
        "    \n",
        "    def train(self, num_epochs, save_path='best_model.pth'):\n",
        "        \"\"\"\n",
        "        Full training loop\n",
        "        \"\"\"\n",
        "        best_val_loss = float('inf')\n",
        "        patience_counter = 0\n",
        "        max_patience = 5\n",
        "        \n",
        "        print(f\"Starting training for {num_epochs} epochs...\")\n",
        "        print(f\"Device: {self.device}\")\n",
        "        print(f\"Model parameters: {sum(p.numel() for p in self.model.parameters()):,}\")\n",
        "        \n",
        "        for epoch in range(1, num_epochs + 1):\n",
        "            start_time = time.time()\n",
        "            \n",
        "            # Training\n",
        "            train_loss, train_perplexity = self.train_epoch(epoch)\n",
        "            \n",
        "            # Validation\n",
        "            val_loss, val_perplexity = self.validate()\n",
        "            \n",
        "            # Learning rate scheduling\n",
        "            self.scheduler.step(val_loss)\n",
        "            \n",
        "            # Save metrics\n",
        "            self.train_losses.append(train_loss)\n",
        "            self.val_losses.append(val_loss)\n",
        "            self.train_perplexities.append(train_perplexity)\n",
        "            self.val_perplexities.append(val_perplexity)\n",
        "            \n",
        "            epoch_time = time.time() - start_time\n",
        "            \n",
        "            print(f'Epoch {epoch}/{num_epochs}:')\n",
        "            print(f'  Train Loss: {train_loss:.4f}, Train Perplexity: {train_perplexity:.4f}')\n",
        "            print(f'  Val Loss: {val_loss:.4f}, Val Perplexity: {val_perplexity:.4f}')\n",
        "            print(f'  Time: {epoch_time:.2f}s, LR: {self.optimizer.param_groups[0][\"lr\"]:.6f}')\n",
        "            \n",
        "            # Save best model\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                patience_counter = 0\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': self.model.state_dict(),\n",
        "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                    'train_losses': self.train_losses,\n",
        "                    'val_losses': self.val_losses,\n",
        "                    'train_perplexities': self.train_perplexities,\n",
        "                    'val_perplexities': self.val_perplexities,\n",
        "                    'best_val_loss': best_val_loss\n",
        "                }, save_path)\n",
        "                print(f'  New best model saved! Val Loss: {val_loss:.4f}')\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                \n",
        "            # Early stopping\n",
        "            if patience_counter >= max_patience:\n",
        "                print(f'Early stopping after {epoch} epochs')\n",
        "                break\n",
        "                \n",
        "            print('-' * 60)\n",
        "        \n",
        "        print('Training completed!')\n",
        "        return self.train_losses, self.val_losses\n",
        "    \n",
        "    def plot_training_curves(self, save_path='training_curves.png'):\n",
        "        \"\"\"\n",
        "        Plot training and validation curves\n",
        "        \"\"\"\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "        \n",
        "        # Loss curves\n",
        "        ax1.plot(self.train_losses, label='Train Loss', color='blue')\n",
        "        ax1.plot(self.val_losses, label='Validation Loss', color='red')\n",
        "        ax1.set_xlabel('Epoch')\n",
        "        ax1.set_ylabel('Loss')\n",
        "        ax1.set_title('Training and Validation Loss')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True)\n",
        "        \n",
        "        # Perplexity curves\n",
        "        ax2.plot(self.train_perplexities, label='Train Perplexity', color='blue')\n",
        "        ax2.plot(self.val_perplexities, label='Validation Perplexity', color='red')\n",
        "        ax2.set_xlabel('Epoch')\n",
        "        ax2.set_ylabel('Perplexity')\n",
        "        ax2.set_title('Training and Validation Perplexity')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "def load_model(model_class, model_path, src_vocab_size, tgt_vocab_size, device='cuda', **model_kwargs):\n",
        "    \"\"\"\n",
        "    Load a trained model\n",
        "    \"\"\"\n",
        "    model = model_class(src_vocab_size=src_vocab_size, tgt_vocab_size=tgt_vocab_size, **model_kwargs)\n",
        "    \n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.to(device)\n",
        "    \n",
        "    return model, checkpoint\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header_evaluation"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evaluation"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import editdistance\n",
        "from collections import Counter\n",
        "import math\n",
        "import re\n",
        "\n",
        "class Evaluator:\n",
        "    \"\"\"\n",
        "    Evaluation metrics for translation quality\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model, test_loader, src_tokenizer, tgt_tokenizer, device='cuda'):\n",
        "        self.model = model\n",
        "        self.test_loader = test_loader\n",
        "        self.src_tokenizer = src_tokenizer\n",
        "        self.tgt_tokenizer = tgt_tokenizer\n",
        "        self.device = device\n",
        "    \n",
        "    def calculate_bleu(self, references, hypotheses, n=4):\n",
        "        \"\"\"\n",
        "        Calculate BLEU score\n",
        "        \"\"\"\n",
        "        def get_ngrams(tokens, n):\n",
        "            if len(tokens) < n:\n",
        "                return Counter()\n",
        "            ngrams = []\n",
        "            for i in range(len(tokens) - n + 1):\n",
        "                ngrams.append(tuple(tokens[i:i+n]))\n",
        "            return Counter(ngrams)\n",
        "        \n",
        "        def calculate_precision(ref_tokens, hyp_tokens, n):\n",
        "            ref_ngrams = get_ngrams(ref_tokens, n)\n",
        "            hyp_ngrams = get_ngrams(hyp_tokens, n)\n",
        "            \n",
        "            if not hyp_ngrams:\n",
        "                return 0.0\n",
        "            \n",
        "            matches = 0\n",
        "            for ngram, count in hyp_ngrams.items():\n",
        "                matches += min(count, ref_ngrams.get(ngram, 0))\n",
        "            \n",
        "            return matches / sum(hyp_ngrams.values())\n",
        "        \n",
        "        # Calculate precision for each n-gram order\n",
        "        precisions = []\n",
        "        for i in range(1, n + 1):\n",
        "            precision_scores = []\n",
        "            for ref, hyp in zip(references, hypotheses):\n",
        "                ref_tokens = ref.split()\n",
        "                hyp_tokens = hyp.split()\n",
        "                precision = calculate_precision(ref_tokens, hyp_tokens, i)\n",
        "                precision_scores.append(precision)\n",
        "            precisions.append(np.mean(precision_scores))\n",
        "        \n",
        "        # Calculate brevity penalty\n",
        "        ref_lengths = [len(ref.split()) for ref in references]\n",
        "        hyp_lengths = [len(hyp.split()) for hyp in hypotheses]\n",
        "        \n",
        "        ref_len = sum(ref_lengths)\n",
        "        hyp_len = sum(hyp_lengths)\n",
        "        \n",
        "        if hyp_len > ref_len:\n",
        "            bp = 1.0\n",
        "        else:\n",
        "            bp = math.exp(1 - ref_len / hyp_len) if hyp_len > 0 else 0.0\n",
        "        \n",
        "        # Calculate BLEU score\n",
        "        if any(p == 0 for p in precisions):\n",
        "            return 0.0\n",
        "        \n",
        "        geometric_mean = math.exp(sum(math.log(p) for p in precisions) / len(precisions))\n",
        "        bleu = bp * geometric_mean\n",
        "        \n",
        "        return bleu\n",
        "    \n",
        "    def calculate_cer(self, references, hypotheses):\n",
        "        \"\"\"\n",
        "        Calculate Character Error Rate\n",
        "        \"\"\"\n",
        "        total_chars = 0\n",
        "        total_errors = 0\n",
        "        \n",
        "        for ref, hyp in zip(references, hypotheses):\n",
        "            ref_chars = list(ref.replace(' ', ''))\n",
        "            hyp_chars = list(hyp.replace(' ', ''))\n",
        "            \n",
        "            total_chars += len(ref_chars)\n",
        "            total_errors += editdistance.eval(ref_chars, hyp_chars)\n",
        "        \n",
        "        return total_errors / total_chars if total_chars > 0 else 1.0\n",
        "    \n",
        "    def calculate_perplexity(self):\n",
        "        \"\"\"\n",
        "        Calculate perplexity on test set\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        total_tokens = 0\n",
        "        criterion = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(self.test_loader, desc='Calculating perplexity'):\n",
        "                src = batch['src'].to(self.device)\n",
        "                tgt = batch['tgt'].to(self.device)\n",
        "                src_lengths = batch['src_lengths'].to(self.device)\n",
        "                \n",
        "                outputs = self.model(src, tgt, src_lengths, teacher_forcing_ratio=0)\n",
        "                \n",
        "                outputs = outputs[:, 1:].contiguous().view(-1, outputs.size(-1))\n",
        "                targets = tgt[:, 1:].contiguous().view(-1)\n",
        "                \n",
        "                loss = criterion(outputs, targets)\n",
        "                total_loss += loss.item()\n",
        "                total_tokens += targets.ne(0).sum().item()\n",
        "        \n",
        "        avg_loss = total_loss / len(self.test_loader)\n",
        "        perplexity = np.exp(total_loss * len(self.test_loader) / total_tokens)\n",
        "        \n",
        "        return perplexity\n",
        "    \n",
        "    def generate_translations(self, num_samples=None):\n",
        "        \"\"\"\n",
        "        Generate translations for the test set\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        references = []\n",
        "        hypotheses = []\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for batch_idx, batch in enumerate(tqdm(self.test_loader, desc='Generating translations')):\n",
        "                if num_samples and batch_idx * self.test_loader.batch_size >= num_samples:\n",
        "                    break\n",
        "                \n",
        "                src = batch['src'].to(self.device)\n",
        "                tgt = batch['tgt'].to(self.device)\n",
        "                \n",
        "                # Generate translations\n",
        "                generated = self.model.inference(src, self.src_tokenizer, self.tgt_tokenizer)\n",
        "                \n",
        "                # Decode sequences\n",
        "                for i in range(src.size(0)):\n",
        "                    # Reference (remove SOS and EOS tokens)\n",
        "                    ref_tokens = tgt[i].cpu().numpy()\n",
        "                    ref_tokens = ref_tokens[1:]  # Remove SOS\n",
        "                    eos_idx = np.where(ref_tokens == self.tgt_tokenizer.vocab['<eos>'])[0]\n",
        "                    if len(eos_idx) > 0:\n",
        "                        ref_tokens = ref_tokens[:eos_idx[0]]\n",
        "                    \n",
        "                    ref_text = self.tgt_tokenizer.decode(ref_tokens.tolist())\n",
        "                    \n",
        "                    # Hypothesis\n",
        "                    if generated.size(1) > 0:\n",
        "                        hyp_tokens = generated[i].cpu().numpy()\n",
        "                        eos_idx = np.where(hyp_tokens == self.tgt_tokenizer.vocab['<eos>'])[0]\n",
        "                        if len(eos_idx) > 0:\n",
        "                            hyp_tokens = hyp_tokens[:eos_idx[0]]\n",
        "                        hyp_text = self.tgt_tokenizer.decode(hyp_tokens.tolist())\n",
        "                    else:\n",
        "                        hyp_text = \"\"\n",
        "                    \n",
        "                    references.append(ref_text)\n",
        "                    hypotheses.append(hyp_text)\n",
        "        \n",
        "        return references, hypotheses\n",
        "    \n",
        "    def evaluate(self, num_samples=None):\n",
        "        \"\"\"\n",
        "        Comprehensive evaluation\n",
        "        \"\"\"\n",
        "        print(\"Starting evaluation...\")\n",
        "        \n",
        "        # Generate translations\n",
        "        references, hypotheses = self.generate_translations(num_samples)\n",
        "        \n",
        "        # Calculate metrics\n",
        "        bleu_score = self.calculate_bleu(references, hypotheses)\n",
        "        cer = self.calculate_cer(references, hypotheses)\n",
        "        perplexity = self.calculate_perplexity()\n",
        "        \n",
        "        # Print results\n",
        "        print(f\"\\nEvaluation Results:\")\n",
        "        print(f\"  BLEU Score: {bleu_score:.4f}\")\n",
        "        print(f\"  Character Error Rate: {cer:.4f}\")\n",
        "        print(f\"  Perplexity: {perplexity:.4f}\")\n",
        "        \n",
        "        # Show some examples\n",
        "        print(f\"\\nSample Translations:\")\n",
        "        for i in range(min(5, len(references))):\n",
        "            print(f\"\\n{i+1}.\")\n",
        "            print(f\"Reference: {references[i]}\")\n",
        "            print(f\"Generated: {hypotheses[i]}\")\n",
        "        \n",
        "        return {\n",
        "            'bleu': bleu_score,\n",
        "            'cer': cer,\n",
        "            'perplexity': perplexity,\n",
        "            'references': references,\n",
        "            'hypotheses': hypotheses\n",
        "        }\n",
        "\n",
        "def translate_text(model, text, src_tokenizer, tgt_tokenizer, device='cuda', max_length=50):\n",
        "    \"\"\"\n",
        "    Translate a single text using the trained model\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # Tokenize input\n",
        "    src_tokens = src_tokenizer.encode(text)\n",
        "    src_tensor = torch.tensor([src_tokens], dtype=torch.long).to(device)\n",
        "    \n",
        "    # Generate translation\n",
        "    with torch.no_grad():\n",
        "        generated = model.inference(src_tensor, src_tokenizer, tgt_tokenizer, max_length)\n",
        "    \n",
        "    # Decode output\n",
        "    if generated.size(1) > 0:\n",
        "        output_tokens = generated[0].cpu().numpy()\n",
        "        eos_idx = np.where(output_tokens == tgt_tokenizer.vocab['<eos>'])[0]\n",
        "        if len(eos_idx) > 0:\n",
        "            output_tokens = output_tokens[:eos_idx[0]]\n",
        "        translation = tgt_tokenizer.decode(output_tokens.tolist())\n",
        "    else:\n",
        "        translation = \"\"\n",
        "    \n",
        "    return translation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "execution_header"
      },
      "source": [
        "## Training Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "execution"
      },
      "outputs": [],
      "source": [
        "# Main training execution\n",
        "\n",
        "# Set up experiment configuration\n",
        "config = {\n",
        "    'seed': 42,\n",
        "    'embed_dim': 256,\n",
        "    'hidden_dim': 512,\n",
        "    'encoder_layers': 2,\n",
        "    'decoder_layers': 4,\n",
        "    'dropout': 0.3,\n",
        "    'learning_rate': 1e-3,\n",
        "    'batch_size': 32,  # Reduced for Colab free tier\n",
        "    'src_vocab_size': 8000,\n",
        "    'tgt_vocab_size': 8000,\n",
        "    'num_epochs': 10  # Reduced for Colab free tier\n",
        "}\n",
        "\n",
        "print(\"Starting Urdu to Roman Urdu NMT Training...\")\n",
        "print(f\"Configuration: {config}\")\n",
        "\n",
        "# 1. Load and preprocess data\n",
        "print(\"\\n1. Loading and preprocessing data...\")\n",
        "\n",
        "# QUICK FIX: Find the correct dataset path (avoiding __MACOSX)\n",
        "print(\"ðŸ” Searching for correct dataset...\")\n",
        "dataset_path = None\n",
        "\n",
        "# Check if extraction was successful - SKIP __MACOSX folders\n",
        "if os.path.exists('/content/dataset_extracted'):\n",
        "    for root, dirs, files in os.walk('/content/dataset_extracted'):\n",
        "        # Skip __MACOSX directories - they contain corrupted files\n",
        "        if '__MACOSX' in root:\n",
        "            continue\n",
        "            \n",
        "        # Look for poet directories\n",
        "        poet_dirs = [d for d in dirs if any(poet in d for poet in ['mirza-ghalib', 'ahmad-faraz', 'allama-iqbal'])]\n",
        "        if poet_dirs:\n",
        "            dataset_path = root\n",
        "            print(f\"âœ… Found valid dataset at: {dataset_path}\")\n",
        "            print(f\"ðŸ“š Sample poets: {poet_dirs[:3]}\")\n",
        "            break\n",
        "\n",
        "if dataset_path:\n",
        "    print(f\"âœ… Using dataset: {dataset_path}\")\n",
        "    urdu_texts, roman_texts = load_dataset(dataset_path)\n",
        "else:\n",
        "    print(\"âŒ Could not find valid dataset! Trying alternative extraction...\")\n",
        "    \n",
        "    # Alternative: Extract directly without __MACOSX\n",
        "    import zipfile\n",
        "    zip_path = '/content/urdu_ghazals_rekhta/dataset/dataset.zip'\n",
        "    extract_to = '/content/dataset_clean'\n",
        "    \n",
        "    if os.path.exists(zip_path):\n",
        "        print(f\"ðŸ“¦ Re-extracting {zip_path} without __MACOSX...\")\n",
        "        os.makedirs(extract_to, exist_ok=True)\n",
        "        \n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            for member in zip_ref.infolist():\n",
        "                # Skip __MACOSX files\n",
        "                if '__MACOSX' not in member.filename and not member.filename.startswith('.'):\n",
        "                    zip_ref.extract(member, extract_to)\n",
        "        \n",
        "        # Find dataset in clean extraction\n",
        "        for root, dirs, files in os.walk(extract_to):\n",
        "            if any(poet in dirs for poet in ['mirza-ghalib', 'ahmad-faraz', 'allama-iqbal']):\n",
        "                dataset_path = root\n",
        "                print(f\"âœ… Found clean dataset at: {dataset_path}\")\n",
        "                break\n",
        "        \n",
        "        if dataset_path:\n",
        "            urdu_texts, roman_texts = load_dataset(dataset_path)\n",
        "        else:\n",
        "            print(\"âŒ Still no valid dataset found!\")\n",
        "            urdu_texts, roman_texts = [], []\n",
        "    else:\n",
        "        print(\"âŒ Zip file not found!\")\n",
        "        urdu_texts, roman_texts = [], []\n",
        "\n",
        "train_pairs, val_pairs, test_pairs = clean_and_split_data(\n",
        "    urdu_texts, roman_texts,\n",
        "    train_ratio=0.5, val_ratio=0.25, test_ratio=0.25\n",
        ")\n",
        "\n",
        "# Check if we have data to proceed\n",
        "if len(train_pairs) == 0:\n",
        "    print(\"âŒ No training data available! Please check dataset extraction.\")\n",
        "    print(\"ðŸ›‘ STOPPING EXECUTION - No data to train on!\")\n",
        "    exit()  # Stop here\n",
        "else:\n",
        "    print(f\"âœ… Ready to proceed with {len(train_pairs)} training pairs\")\n",
        "\n",
        "# 2. Create tokenizers\n",
        "print(\"\\n2. Training tokenizers...\")\n",
        "src_tokenizer, tgt_tokenizer = create_tokenizers(\n",
        "    train_pairs,\n",
        "    src_vocab_size=config['src_vocab_size'],\n",
        "    tgt_vocab_size=config['tgt_vocab_size']\n",
        ")\n",
        "\n",
        "# 3. Create data loaders\n",
        "print(\"\\n3. Creating data loaders...\")\n",
        "train_loader, val_loader, test_loader = create_data_loaders(\n",
        "    train_pairs, val_pairs, test_pairs,\n",
        "    src_tokenizer, tgt_tokenizer,\n",
        "    batch_size=config['batch_size']\n",
        ")\n",
        "\n",
        "# 4. Create model\n",
        "print(\"\\n4. Creating model...\")\n",
        "model = Seq2SeqModel(\n",
        "    src_vocab_size=src_tokenizer.get_vocab_size(),\n",
        "    tgt_vocab_size=tgt_tokenizer.get_vocab_size(),\n",
        "    embed_dim=config['embed_dim'],\n",
        "    hidden_dim=config['hidden_dim'],\n",
        "    encoder_layers=config['encoder_layers'],\n",
        "    decoder_layers=config['decoder_layers'],\n",
        "    dropout=config['dropout']\n",
        ")\n",
        "\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "# 5. Train model\n",
        "print(\"\\n5. Starting training...\")\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    src_tokenizer=src_tokenizer,\n",
        "    tgt_tokenizer=tgt_tokenizer,\n",
        "    lr=config['learning_rate'],\n",
        "    device=device\n",
        ")\n",
        "\n",
        "train_losses, val_losses = trainer.train(\n",
        "    num_epochs=config['num_epochs'],\n",
        "    save_path='best_model.pth'\n",
        ")\n",
        "\n",
        "# 6. Plot training curves\n",
        "trainer.plot_training_curves('training_curves.png')\n",
        "\n",
        "# 7. Evaluate model\n",
        "print(\"\\n6. Evaluating model...\")\n",
        "evaluator = Evaluator(model, test_loader, src_tokenizer, tgt_tokenizer, device)\n",
        "results = evaluator.evaluate(num_samples=200)  # Reduced for speed\n",
        "\n",
        "print(\"\\n=\" * 60)\n",
        "print(\"TRAINING COMPLETED!\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Final Results:\")\n",
        "print(f\"  BLEU Score: {results['bleu']:.4f}\")\n",
        "print(f\"  Character Error Rate: {results['cer']:.4f}\")\n",
        "print(f\"  Perplexity: {results['perplexity']:.4f}\")\n",
        "\n",
        "# 8. Test with sample translations\n",
        "print(\"\\n7. Sample translations:\")\n",
        "sample_texts = [\n",
        "    \"Ù…Ø­Ø¨Øª Ù…ÛŒÚº Ù†ÛÛŒÚº ÛÛ’ ÙØ±Ù‚ Ø¬ÛŒÙ†Û’ Ø§ÙˆØ± Ù…Ø±Ù†Û’ Ú©Ø§\",\n",
        "    \"Ø¯Ù„ ÛÛŒ ØªÙˆ ÛÛ’ Ù†Û Ø³Ù†Ú¯ Ùˆ Ø®Ø´Øª Ø¯Ø±Ø¯ Ø³Û’ Ø¨Ú¾Ø± Ù†Û Ø¢Ø¦Û’ Ú©ÛŒÙˆÚº\",\n",
        "    \"ÛØ²Ø§Ø±ÙˆÚº Ø®ÙˆØ§ÛØ´ÛŒÚº Ø§ÛŒØ³ÛŒ Ú©Û ÛØ± Ø®ÙˆØ§ÛØ´ Ù¾Û Ø¯Ù… Ù†Ú©Ù„Û’\"\n",
        "]\n",
        "\n",
        "for text in sample_texts:\n",
        "    translation = translate_text(model, text, src_tokenizer, tgt_tokenizer, device)\n",
        "    print(f\"Urdu: {text}\")\n",
        "    print(f\"Roman: {translation}\")\n",
        "    print()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
